'codebert_md.bat' is not recognized as an internal or external command,
operable program or batch file.

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py 	--output_folder_name=md 	--output_dir=./saved_models 	--model_type=roberta 	--tokenizer_name=microsoft/codebert-base 	--model_name_or_path=microsoft/codebert-base 	--do_train 	--train_data_file=../dataset/devign/train.jsonl 	--eval_data_file=../dataset/devign/valid.jsonl 	--test_data_file=../dataset/devign/test.jsonl 	--epoch 5 	--block_size 400 	--train_batch_size 32 	--eval_batch_size 64 	--learning_rate 2e-5 	--max_grad_norm 1.0 	--evaluate_during_training 	--seed 12 
08/28/2022 22:41:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/28/2022 22:41:44 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/devign/train.jsonl', output_folder_name='md', output_dir='./saved_models', eval_data_file='../dataset/devign/valid.jsonl', test_data_file='../dataset/devign/test.jsonl', model_type='roberta', model_name_or_path='microsoft/codebert-base', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/codebert-base', cache_dir='', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=12, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=32, per_gpu_eval_batch_size=64, start_epoch=0, start_step=0)
Traceback (most recent call last):
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 595, in <module>
    main()
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 562, in main
    train_dataset = TextDataset(tokenizer, args,args.train_data_file)
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 94, in __init__
    with open(file_path) as f:
FileNotFoundError: [Errno 2] No such file or directory: '../dataset/devign/train.jsonl'

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py 	--output_folder_name=md 	--output_dir=./saved_models 	--model_type=roberta 	--tokenizer_name=microsoft/codebert-base 	--model_name_or_path=microsoft/codebert-base 	--do_test 	--train_data_file=../dataset/devign/train.jsonl 	--eval_data_file=../dataset/devign/valid.jsonl 	--test_data_file=../dataset/devign/test.jsonl 	--epoch 5 	--block_size 400 	--train_batch_size 32 	--eval_batch_size 64 	--learning_rate 2e-5 	--max_grad_norm 1.0 	--evaluate_during_training 	--seed 12 
08/28/2022 22:41:46 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/28/2022 22:41:49 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/devign/train.jsonl', output_folder_name='md', output_dir='./saved_models', eval_data_file='../dataset/devign/valid.jsonl', test_data_file='../dataset/devign/test.jsonl', model_type='roberta', model_name_or_path='microsoft/codebert-base', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/codebert-base', cache_dir='', block_size=400, do_train=False, do_eval=False, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=12, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=32, per_gpu_eval_batch_size=64, start_epoch=0, start_step=0)
checkpoint-best-accmd/model.bin
Traceback (most recent call last):
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 595, in <module>
    main()
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 589, in main
    test(args, model, tokenizer)
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 319, in test
    eval_dataset = TextDataset(tokenizer, args,args.test_data_file)
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 94, in __init__
    with open(file_path) as f:
FileNotFoundError: [Errno 2] No such file or directory: '../dataset/devign/test.jsonl'

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py 	--output_folder_name=md 	--output_dir=./saved_models 	--model_type=roberta 	--tokenizer_name=microsoft/codebert-base 	--model_name_or_path=microsoft/codebert-base 	--do_train 	--train_data_file=../dataset/devign/train.jsonl 	--eval_data_file=../dataset/devign/valid.jsonl 	--test_data_file=../dataset/devign/test.jsonl 	--epoch 5 	--block_size 400 	--train_batch_size 32 	--eval_batch_size 64 	--learning_rate 2e-5 	--max_grad_norm 1.0 	--evaluate_during_training 	--seed 12 
08/28/2022 22:46:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/28/2022 22:46:07 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/devign/train.jsonl', output_folder_name='md', output_dir='./saved_models', eval_data_file='../dataset/devign/valid.jsonl', test_data_file='../dataset/devign/test.jsonl', model_type='roberta', model_name_or_path='microsoft/codebert-base', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/codebert-base', cache_dir='', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=12, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=32, per_gpu_eval_batch_size=64, start_epoch=0, start_step=0)
08/28/2022 22:46:43 - INFO - __main__ -   *** Example ***
08/28/2022 22:46:43 - INFO - __main__ -   idx: 0
08/28/2022 22:46:43 - INFO - __main__ -   label: 0
08/28/2022 22:46:43 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_av', '_', 'cold', '_int', '_v', 'd', 'ade', 'c', '_', 'init', '(', 'AV', 'Cod', 'ec', 'Context', '_*', 'av', 'ctx', ')', '_{', '_V', 'D', 'AD', 'ec', 'oder', 'Context', '_*', 'ctx', '_=', '_av', 'ctx', '->', 'priv', '_', 'data', ';', '_struct', '_v', 'da', '_', 'context', '_*', 'v', 'da', '_', 'ctx', '_=', '_&', 'ctx', '->', 'v', 'da', '_', 'ctx', ';', '_OS', 'Status', '_status', ';', '_int', '_ret', ';', '_c', 'tx', '->', 'h', '264', '_', 'initialized', '_=', '_0', ';', '_/*', '_init', '_p', 'ix', '_', 'fm', 'ts', '_of', '_codec', '_*/', '_if', '_(!', 'ff', '_', 'h', '264', '_', 'v', 'da', '_', 'dec', 'oder', '.', 'p', 'ix', '_', 'fm', 'ts', ')', '_{', '_if', '_(', 'k', 'C', 'FC', 'ore', 'Found', 'ation', 'Version', 'Number', '_<', '_k', 'C', 'FC', 'ore', 'Found', 'ation', 'Version', 'Number', '10', '_', '7', ')', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'dec', 'oder', '.', 'p', 'ix', '_', 'fm', 'ts', '_=', '_v', 'da', '_', 'p', 'ix', 'fm', 'ts', '_', 'pri', 'or', '_', '10', '_', '7', ';', '_else', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'dec', 'oder', '.', 'p', 'ix', '_', 'fm', 'ts', '_=', '_v', 'da', '_', 'p', 'ix', 'fm', 'ts', ';', '_}', '_/*', '_init', '_v', 'da', '_*/', '_mem', 'set', '(', 'v', 'da', '_', 'ctx', ',', '_0', ',', '_sizeof', '(', 'struct', '_v', 'da', '_', 'context', '));', '_v', 'da', '_', 'ctx', '->', 'width', '_=', '_av', 'ctx', '->', 'width', ';', '_v', 'da', '_', 'ctx', '->', 'height', '_=', '_av', 'ctx', '->', 'height', ';', '_v', 'da', '_', 'ctx', '->', 'format', '_=', "_'", 'av', 'c', '1', "';", '_v', 'da', '_', 'ctx', '->', 'use', '_', 'sync', '_', 'dec', 'oding', '_=', '_1', ';', '_v', 'da', '_', 'ctx', '->', 'use', '_', 'ref', '_', 'buffer', '_=', '_1', ';', '_c', 'tx', '->', 'p', 'ix', '_', 'f', 'mt', '_=', '_av', 'ctx', '->', 'get', '_', 'format', '(', 'av', 'ctx', ',', '_av', 'ctx', '->', 'cod', 'ec', '->', 'p', 'ix', '_', 'fm', 'ts', ');', '_switch', '_(', 'ctx', '->', 'p', 'ix', '_', 'f', 'mt', ')', '_{', '_case', '_AV', '_', 'P', 'IX', '_', 'F', 'MT', '_', 'U', 'Y', 'V', 'Y', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'p', 'ix', '_', 'f', 'mt', '_', 'type', '_=', "_'", '2', 'v', 'uy', "';", '_break', ';', '_case', '_AV', '_', 'P', 'IX', '_', 'F', 'MT', '_', 'Y', 'U', 'Y', 'V', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'p', 'ix', '_', 'f', 'mt', '_', 'type', '_=', "_'", 'yu', 'vs', "';", '_break', ';', '_case', '_AV', '_', 'P', 'IX', '_', 'F', 'MT', '_', 'NV', '12', ':', '</s>']
08/28/2022 22:46:43 - INFO - __main__ -   input_ids: 0 42653 6402 1215 33912 6979 748 417 1829 438 1215 25153 1640 10612 47436 3204 48522 1009 1469 49575 43 25522 468 495 2606 3204 15362 48522 1009 49575 5457 6402 49575 46613 25943 1215 23687 131 29916 748 6106 1215 46796 1009 705 6106 1215 49575 5457 359 49575 46613 705 6106 1215 49575 131 8192 47731 2194 131 6979 5494 131 740 43820 46613 298 29137 1215 49722 5457 321 131 48565 45511 181 3181 1215 40523 1872 9 45797 48404 114 48209 3145 1215 298 29137 1215 705 6106 1215 11127 15362 4 642 3181 1215 40523 1872 43 25522 114 36 330 347 5268 1688 29991 1258 47322 43623 28696 449 347 5268 1688 29991 1258 47322 43623 698 1215 406 43 48400 1215 298 29137 1215 705 6106 1215 11127 15362 4 642 3181 1215 40523 1872 5457 748 6106 1215 642 3181 40523 1872 1215 13718 368 1215 698 1215 406 131 1493 48400 1215 298 29137 1215 705 6106 1215 11127 15362 4 642 3181 1215 40523 1872 5457 748 6106 1215 642 3181 40523 1872 131 35524 48565 45511 748 6106 48404 26012 8738 1640 705 6106 1215 49575 6 321 6 49907 1640 25384 748 6106 1215 46796 48749 748 6106 1215 49575 46613 36097 5457 6402 49575 46613 36097 131 748 6106 1215 49575 46613 37009 5457 6402 49575 46613 37009 131 748 6106 1215 49575 46613 34609 5457 128 1469 438 134 23500 748 6106 1215 49575 46613 3698 1215 45176 1215 11127 19519 5457 112 131 748 6106 1215 49575 46613 3698 1215 13043 1215 47438 5457 112 131 740 43820 46613 642 3181 1215 506 16100 5457 6402 49575 46613 6460 1215 34609 1640 1469 49575 6 6402 49575 46613 29659 3204 46613 642 3181 1215 40523 1872 4397 5405 36 49575 46613 642 3181 1215 506 16100 43 25522 403 17307 1215 510 9482 1215 597 11674 1215 791 975 846 975 37319 35 748 6106 1215 49575 46613 38635 1215 642 3181 1215 506 16100 1215 12528 5457 128 176 705 5781 23500 1108 131 403 17307 1215 510 9482 1215 597 11674 1215 975 791 975 846 37319 35 748 6106 1215 49575 46613 38635 1215 642 3181 1215 506 16100 1215 12528 5457 128 29159 15597 23500 1108 131 403 17307 1215 510 9482 1215 597 11674 1215 31668 1092 35 2
08/28/2022 22:46:43 - INFO - __main__ -   *** Example ***
08/28/2022 22:46:43 - INFO - __main__ -   idx: 1
08/28/2022 22:46:43 - INFO - __main__ -   label: 0
08/28/2022 22:46:43 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_int', '_trans', 'code', '(', 'AV', 'Format', 'Context', '_**', 'output', '_', 'files', ',', '_int', '_n', 'b', '_', 'output', '_', 'files', ',', '_Input', 'File', '_*', 'input', '_', 'files', ',', '_int', '_n', 'b', '_', 'input', '_', 'files', ',', '_Stream', 'Map', '_*', 'stream', '_', 'maps', ',', '_int', '_n', 'b', '_', 'stream', '_', 'maps', ')', '_{', '_int', '_ret', '_=', '_0', ',', '_i', ',', '_j', ',', '_k', ',', '_n', ',', '_n', 'b', '_', 'ost', 'ream', 's', '_=', '_0', ',', '_step', ';', '_AV', 'Format', 'Context', '_*', 'is', ',', '_*', 'os', ';', '_AV', 'Cod', 'ec', 'Context', '_*', 'cod', 'ec', ',', '_*', 'ic', 'od', 'ec', ';', '_Output', 'Stream', '_*', 'ost', ',', '_**', 'ost', '_', 'table', '_=', '_NULL', ';', '_Input', 'Stream', '_*', 'ist', ';', '_char', '_error', '[', '1024', '];', '_int', '_key', ';', '_int', '_want', '_', 'sd', 'p', '_=', '_1', ';', '_uint', '8', '_', 't', '_no', '_', 'pack', 'et', '[', 'MAX', '_', 'FIL', 'ES', ']=', '{', '0', '};', '_int', '_no', '_', 'pack', 'et', '_', 'count', '=', '0', ';', '_int', '_n', 'b', '_', 'frame', '_', 'th', 'reshold', '[', 'AV', 'MED', 'IA', '_', 'TYPE', '_', 'NB', ']=', '{', '0', '};', '_int', '_n', 'b', '_', 'stream', 's', '[', 'AV', 'MED', 'IA', '_', 'TYPE', '_', 'NB', ']=', '{', '0', '};', '_if', '_(', 'rate', '_', 'em', 'u', ')', '_for', '_(', 'i', '_=', '_0', ';', '_i', '_<', '_n', 'b', '_', 'input', '_', 'stream', 's', ';', '_i', '++)', '_input', '_', 'stream', 's', '[', 'i', '].', 'start', '_=', '_av', '_', 'get', 'time', '();', '_/*', '_output', '_stream', '_init', '_*/', '_n', 'b', '_', 'ost', 'ream', 's', '_=', '_0', ';', '_for', '(', 'i', '=', '0', ';', 'i', '<', 'nb', '_', 'output', '_', 'files', ';', 'i', '++)', '_{', '_os', '_=', '_output', '_', 'files', '[', 'i', '];', '_if', '_(!', 'os', '->', 'nb', '_', 'stream', 's', '_&&', '_!', '(', 'os', '->', 'o', 'format', '->', 'flags', '_&', '_AV', 'F', 'MT', '_', 'N', 'OST', 'REAM', 'S', '))', '_{', '_av', '_', 'dump', '_', 'format', '(', 'output', '_', 'files', '[', 'i', '],', '_i', ',', '_output', '_', 'files', '[', 'i', ']', '->', 'filename', ',', '_1', ');', '_f', 'printf', '(', 'st', 'der', 'r', ',', '_"', 'Output', '_file', '_#', '%', 'd', '_does', '_not', '_contain', '_any', '_stream', '\\', 'n', '",', '_i', ');', '_ret', '_=', '_A', 'VER', 'ROR', '(', 'E', 'IN', 'VAL', ');', '_goto', '_fail', ';', '_}', '_n', 'b', '_', 'ost', 'ream', 's', '_+=', '_os', '->', 'nb', '_', 'stream', 's', ';', '_}', '_if', '_(', 'nb', '_', 'stream', '_', 'maps', '_>', '_0', '_&&', '_n', 'b', '_', 'stream', '_', 'maps', '_!=', '_n', 'b', '</s>']
08/28/2022 22:46:43 - INFO - __main__ -   input_ids: 0 42653 6979 6214 20414 1640 10612 48587 48522 13540 46234 1215 42018 6 6979 295 428 1215 46234 1215 42018 6 41327 9966 1009 46797 1215 42018 6 6979 295 428 1215 46797 1215 42018 6 16183 41151 1009 8656 1215 44754 6 6979 295 428 1215 8656 1215 44754 43 25522 6979 5494 5457 321 6 939 6 1236 6 449 6 295 6 295 428 1215 2603 26930 29 5457 321 6 1149 131 17307 48587 48522 1009 354 6 1009 366 131 17307 47436 3204 48522 1009 29659 3204 6 1009 636 1630 3204 131 38252 36757 1009 2603 6 13540 2603 1215 14595 5457 48955 131 41327 36757 1009 661 131 16224 5849 10975 47477 44082 6979 762 131 6979 236 1215 28045 642 5457 112 131 49315 398 1215 90 117 1215 12486 594 10975 30187 1215 46008 1723 49659 45152 288 49423 6979 117 1215 12486 594 1215 11432 5214 288 131 6979 295 428 1215 26061 1215 212 45749 10975 10612 32653 2889 1215 48710 1215 20485 49659 45152 288 49423 6979 295 428 1215 8656 29 10975 10612 32653 2889 1215 48710 1215 20485 49659 45152 288 49423 114 36 7954 1215 991 257 43 13 36 118 5457 321 131 939 28696 295 428 1215 46797 1215 8656 29 131 939 49346 8135 1215 8656 29 10975 118 8174 13124 5457 6402 1215 6460 958 47006 48565 4195 4615 45511 48404 295 428 1215 2603 26930 29 5457 321 131 13 1640 118 5214 288 131 118 41552 40460 1215 46234 1215 42018 131 118 49346 25522 11988 5457 4195 1215 42018 10975 118 44082 114 48209 366 46613 40460 1215 8656 29 48200 27785 1640 366 46613 139 34609 46613 46760 359 17307 597 11674 1215 487 13556 28057 104 35122 25522 6402 1215 46593 1215 34609 1640 46234 1215 42018 10975 118 7479 939 6 4195 1215 42018 10975 118 742 46613 49451 6 112 4397 856 49775 1640 620 3624 338 6 22 48293 2870 849 207 417 473 45 5585 143 4615 37457 282 1297 939 4397 5494 5457 83 9847 45055 1640 717 2444 39766 4397 49325 5998 131 35524 295 428 1215 2603 26930 29 49371 11988 46613 40460 1215 8656 29 131 35524 114 36 40460 1215 8656 1215 44754 8061 321 48200 295 428 1215 8656 1215 44754 49333 295 428 2
08/28/2022 22:46:43 - INFO - __main__ -   *** Example ***
08/28/2022 22:46:43 - INFO - __main__ -   idx: 2
08/28/2022 22:46:43 - INFO - __main__ -   label: 0
08/28/2022 22:46:43 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_void', '_v', '4', 'l', '2', '_', 'free', '_', 'buffer', '(', 'void', '_*', 'op', 'aque', ',', '_uint', '8', '_', 't', '_*', 'un', 'used', ')', '_{', '_V', '4', 'L', '2', 'Buffer', '*', '_av', 'buf', '_=', '_opaque', ';', '_V', '4', 'L', '2', 'm', '2', 'm', 'Context', '_*', 's', '_=', '_buf', '_', 'to', '_', 'm', '2', 'm', 'ctx', '(', 'av', 'buf', ');', '_if', '_(', 'atomic', '_', 'f', 'etch', '_', 'sub', '(&', 'av', 'buf', '->', 'context', '_', 'ref', 'count', ',', '_1', ')', '_==', '_1', ')', '_{', '_atomic', '_', 'f', 'etch', '_', 'sub', '_', 'expl', 'icit', '(&', 's', '->', 'ref', 'count', ',', '_1', ',', '_memory', '_', 'order', '_', 'ac', 'q', '_', 'rel', ');', '_if', '_(', 's', '->', 're', 'init', ')', '_{', '_if', '_(!', 'atomic', '_', 'load', '(&', 's', '->', 'ref', 'count', '))', '_sem', '_', 'post', '(&', 's', '->', 'ref', 'sync', ');', '_}', '_else', '_if', '_(', 'av', 'buf', '->', 'context', '->', 'stream', 'on', ')', '_ff', '_', 'v', '4', 'l', '2', '_', 'buffer', '_', 'en', 'queue', '(', 'av', 'buf', ');', '_av', '_', 'buffer', '_', 'un', 'ref', '(&', 'av', 'buf', '->', 'context', '_', 'ref', ');', '_}', '_}', '</s>']
08/28/2022 22:46:43 - INFO - __main__ -   input_ids: 0 42653 13842 748 306 462 176 1215 3743 1215 47438 1640 47908 1009 1517 35485 6 49315 398 1215 90 1009 879 6199 43 25522 468 306 574 176 49334 3226 6402 48939 5457 31861 131 468 306 574 176 119 176 119 48522 1009 29 5457 49125 1215 560 1215 119 176 119 49575 1640 1469 48939 4397 114 36 45826 1215 506 29094 1215 10936 49763 1469 48939 46613 46796 1215 13043 11432 6 112 43 45994 112 43 25522 21495 1215 506 29094 1215 10936 1215 23242 17022 49763 29 46613 13043 11432 6 112 6 3783 1215 10337 1215 1043 1343 1215 5982 4397 114 36 29 46613 241 25153 43 25522 114 48209 45826 1215 16204 49763 29 46613 13043 11432 35122 9031 1215 7049 49763 29 46613 13043 45176 4397 35524 1493 114 36 1469 48939 46613 46796 46613 8656 261 43 48400 1215 705 306 462 176 1215 47438 1215 225 48702 1640 1469 48939 4397 6402 1215 47438 1215 879 13043 49763 1469 48939 46613 46796 1215 13043 4397 35524 35524 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
C:\Users\<ANONYMOUS>\Anaconda3\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/28/2022 22:46:43 - INFO - __main__ -   ***** Running training *****
08/28/2022 22:46:43 - INFO - __main__ -     Num examples = 21854
08/28/2022 22:46:43 - INFO - __main__ -     Num Epochs = 5
08/28/2022 22:46:43 - INFO - __main__ -     Instantaneous batch size per GPU = 32
08/28/2022 22:46:43 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
08/28/2022 22:46:43 - INFO - __main__ -     Gradient Accumulation steps = 1
08/28/2022 22:46:43 - INFO - __main__ -     Total optimization steps = 3415
  0%|          | 0/683 [00:00<?, ?it/s]epoch 0 loss 0.71218:   0%|          | 0/683 [00:13<?, ?it/s]epoch 0 loss 0.71218:   0%|          | 1/683 [00:13<2:35:12, 13.65s/it]epoch 0 loss 0.70495:   0%|          | 1/683 [00:14<2:35:12, 13.65s/it]epoch 0 loss 0.70495:   0%|          | 2/683 [00:14<1:07:16,  5.93s/it]epoch 0 loss 0.70928:   0%|          | 2/683 [00:14<1:07:16,  5.93s/it]epoch 0 loss 0.70928:   0%|          | 3/683 [00:14<39:11,  3.46s/it]  epoch 0 loss 0.70289:   0%|          | 3/683 [00:15<39:11,  3.46s/it]epoch 0 loss 0.70289:   1%|          | 4/683 [00:15<25:58,  2.30s/it]epoch 0 loss 0.70326:   1%|          | 4/683 [00:15<25:58,  2.30s/it]epoch 0 loss 0.70326:   1%|          | 5/683 [00:15<18:41,  1.65s/it]epoch 0 loss 0.70092:   1%|          | 5/683 [00:16<18:41,  1.65s/it]epoch 0 loss 0.70092:   1%|          | 6/683 [00:16<14:18,  1.27s/it]epoch 0 loss 0.69913:   1%|          | 6/683 [00:16<14:18,  1.27s/it]epoch 0 loss 0.69913:   1%|1         | 7/683 [00:16<11:32,  1.02s/it]epoch 0 loss 0.69868:   1%|1         | 7/683 [00:17<11:32,  1.02s/it]epoch 0 loss 0.69868:   1%|1         | 8/683 [00:17<09:42,  1.16it/s]epoch 0 loss 0.69998:   1%|1         | 8/683 [00:17<09:42,  1.16it/s]epoch 0 loss 0.69998:   1%|1         | 9/683 [00:17<08:29,  1.32it/s]epoch 0 loss 0.69719:   1%|1         | 9/683 [00:18<08:29,  1.32it/s]epoch 0 loss 0.69719:   1%|1         | 10/683 [00:18<07:39,  1.47it/s]epoch 0 loss 0.69916:   1%|1         | 10/683 [00:18<07:39,  1.47it/s]epoch 0 loss 0.69916:   2%|1         | 11/683 [00:18<07:04,  1.58it/s]epoch 0 loss 0.6971:   2%|1         | 11/683 [00:19<07:04,  1.58it/s] epoch 0 loss 0.6971:   2%|1         | 12/683 [00:19<06:40,  1.68it/s]epoch 0 loss 0.69791:   2%|1         | 12/683 [00:19<06:40,  1.68it/s]epoch 0 loss 0.69791:   2%|1         | 13/683 [00:19<06:24,  1.74it/s]epoch 0 loss 0.69891:   2%|1         | 13/683 [00:20<06:24,  1.74it/s]epoch 0 loss 0.69891:   2%|2         | 14/683 [00:20<06:12,  1.80it/s]epoch 0 loss 0.69824:   2%|2         | 14/683 [00:20<06:12,  1.80it/s]epoch 0 loss 0.69824:   2%|2         | 15/683 [00:20<06:04,  1.83it/s]epoch 0 loss 0.69809:   2%|2         | 15/683 [00:21<06:04,  1.83it/s]epoch 0 loss 0.69809:   2%|2         | 16/683 [00:21<05:58,  1.86it/s]epoch 0 loss 0.69706:   2%|2         | 16/683 [00:21<05:58,  1.86it/s]epoch 0 loss 0.69706:   2%|2         | 17/683 [00:21<05:55,  1.88it/s]epoch 0 loss 0.69795:   2%|2         | 17/683 [00:22<05:55,  1.88it/s]epoch 0 loss 0.69795:   3%|2         | 18/683 [00:22<05:51,  1.89it/s]epoch 0 loss 0.69778:   3%|2         | 18/683 [00:22<05:51,  1.89it/s]epoch 0 loss 0.69778:   3%|2         | 19/683 [00:22<05:49,  1.90it/s]epoch 0 loss 0.69807:   3%|2         | 19/683 [00:23<05:49,  1.90it/s]epoch 0 loss 0.69807:   3%|2         | 20/683 [00:23<05:47,  1.91it/s]epoch 0 loss 0.6981:   3%|2         | 20/683 [00:24<05:47,  1.91it/s] epoch 0 loss 0.6981:   3%|3         | 21/683 [00:24<05:48,  1.90it/s]epoch 0 loss 0.69792:   3%|3         | 21/683 [00:24<05:48,  1.90it/s]epoch 0 loss 0.69792:   3%|3         | 22/683 [00:24<05:46,  1.91it/s]epoch 0 loss 0.69717:   3%|3         | 22/683 [00:25<05:46,  1.91it/s]epoch 0 loss 0.69717:   3%|3         | 23/683 [00:25<05:46,  1.91it/s]epoch 0 loss 0.69679:   3%|3         | 23/683 [00:25<05:46,  1.91it/s]epoch 0 loss 0.69679:   4%|3         | 24/683 [00:25<05:44,  1.91it/s]epoch 0 loss 0.6966:   4%|3         | 24/683 [00:26<05:44,  1.91it/s] epoch 0 loss 0.6966:   4%|3         | 25/683 [00:26<05:42,  1.92it/s]epoch 0 loss 0.696:   4%|3         | 25/683 [00:26<05:42,  1.92it/s] epoch 0 loss 0.696:   4%|3         | 26/683 [00:26<05:40,  1.93it/s]epoch 0 loss 0.69576:   4%|3         | 26/683 [00:27<05:40,  1.93it/s]epoch 0 loss 0.69576:   4%|3         | 27/683 [00:27<05:39,  1.93it/s]epoch 0 loss 0.69548:   4%|3         | 27/683 [00:27<05:39,  1.93it/s]epoch 0 loss 0.69548:   4%|4         | 28/683 [00:27<05:39,  1.93it/s]epoch 0 loss 0.69477:   4%|4         | 28/683 [00:28<05:39,  1.93it/s]epoch 0 loss 0.69477:   4%|4         | 29/683 [00:28<05:39,  1.93it/s]epoch 0 loss 0.6945:   4%|4         | 29/683 [00:28<05:39,  1.93it/s] epoch 0 loss 0.6945:   4%|4         | 30/683 [00:28<05:38,  1.93it/s]epoch 0 loss 0.69476:   4%|4         | 30/683 [00:29<05:38,  1.93it/s]epoch 0 loss 0.69476:   5%|4         | 31/683 [00:29<05:37,  1.93it/s]epoch 0 loss 0.69571:   5%|4         | 31/683 [00:29<05:37,  1.93it/s]epoch 0 loss 0.69571:   5%|4         | 32/683 [00:29<05:36,  1.93it/s]epoch 0 loss 0.69604:   5%|4         | 32/683 [00:30<05:36,  1.93it/s]epoch 0 loss 0.69604:   5%|4         | 33/683 [00:30<05:36,  1.93it/s]epoch 0 loss 0.69557:   5%|4         | 33/683 [00:30<05:36,  1.93it/s]epoch 0 loss 0.69557:   5%|4         | 34/683 [00:30<05:36,  1.93it/s]epoch 0 loss 0.69518:   5%|4         | 34/683 [00:31<05:36,  1.93it/s]epoch 0 loss 0.69518:   5%|5         | 35/683 [00:31<05:36,  1.93it/s]epoch 0 loss 0.69496:   5%|5         | 35/683 [00:31<05:36,  1.93it/s]epoch 0 loss 0.69496:   5%|5         | 36/683 [00:31<05:35,  1.93it/s]epoch 0 loss 0.69524:   5%|5         | 36/683 [00:32<05:35,  1.93it/s]epoch 0 loss 0.69524:   5%|5         | 37/683 [00:32<05:35,  1.93it/s]epoch 0 loss 0.69465:   5%|5         | 37/683 [00:32<05:35,  1.93it/s]epoch 0 loss 0.69465:   6%|5         | 38/683 [00:32<05:34,  1.93it/s]epoch 0 loss 0.69463:   6%|5         | 38/683 [00:33<05:34,  1.93it/s]epoch 0 loss 0.69463:   6%|5         | 39/683 [00:33<05:35,  1.92it/s]epoch 0 loss 0.69437:   6%|5         | 39/683 [00:33<05:35,  1.92it/s]epoch 0 loss 0.69437:   6%|5         | 40/683 [00:33<05:37,  1.90it/s]epoch 0 loss 0.69447:   6%|5         | 40/683 [00:34<05:37,  1.90it/s]epoch 0 loss 0.69447:   6%|6         | 41/683 [00:34<05:35,  1.91it/s]epoch 0 loss 0.6946:   6%|6         | 41/683 [00:34<05:35,  1.91it/s] epoch 0 loss 0.6946:   6%|6         | 42/683 [00:34<05:34,  1.92it/s]epoch 0 loss 0.69529:   6%|6         | 42/683 [00:35<05:34,  1.92it/s]epoch 0 loss 0.69529:   6%|6         | 43/683 [00:35<05:36,  1.90it/s]epoch 0 loss 0.69554:   6%|6         | 43/683 [00:35<05:36,  1.90it/s]epoch 0 loss 0.69554:   6%|6         | 44/683 [00:36<05:34,  1.91it/s]epoch 0 loss 0.69545:   6%|6         | 44/683 [00:36<05:34,  1.91it/s]epoch 0 loss 0.69545:   7%|6         | 45/683 [00:36<05:33,  1.91it/s]epoch 0 loss 0.6955:   7%|6         | 45/683 [00:37<05:33,  1.91it/s] epoch 0 loss 0.6955:   7%|6         | 46/683 [00:37<05:32,  1.92it/s]epoch 0 loss 0.6949:   7%|6         | 46/683 [00:37<05:32,  1.92it/s]epoch 0 loss 0.6949:   7%|6         | 47/683 [00:37<05:31,  1.92it/s]epoch 0 loss 0.69459:   7%|6         | 47/683 [00:38<05:31,  1.92it/s]epoch 0 loss 0.69459:   7%|7         | 48/683 [00:38<05:30,  1.92it/s]epoch 0 loss 0.69439:   7%|7         | 48/683 [00:38<05:30,  1.92it/s]epoch 0 loss 0.69439:   7%|7         | 49/683 [00:38<05:30,  1.92it/s]epoch 0 loss 0.69436:   7%|7         | 49/683 [00:39<05:30,  1.92it/s]epoch 0 loss 0.69436:   7%|7         | 50/683 [00:39<05:29,  1.92it/s]epoch 0 loss 0.69433:   7%|7         | 50/683 [00:39<05:29,  1.92it/s]epoch 0 loss 0.69433:   7%|7         | 51/683 [00:39<05:29,  1.92it/s]epoch 0 loss 0.69412:   7%|7         | 51/683 [00:40<05:29,  1.92it/s]epoch 0 loss 0.69412:   8%|7         | 52/683 [00:40<05:30,  1.91it/s]epoch 0 loss 0.69411:   8%|7         | 52/683 [00:40<05:30,  1.91it/s]epoch 0 loss 0.69411:   8%|7         | 53/683 [00:40<05:31,  1.90it/s]epoch 0 loss 0.69384:   8%|7         | 53/683 [00:41<05:31,  1.90it/s]epoch 0 loss 0.69384:   8%|7         | 54/683 [00:41<05:32,  1.89it/s]epoch 0 loss 0.69376:   8%|7         | 54/683 [00:41<05:32,  1.89it/s]epoch 0 loss 0.69376:   8%|8         | 55/683 [00:41<05:32,  1.89it/s]epoch 0 loss 0.69372:   8%|8         | 55/683 [00:42<05:32,  1.89it/s]epoch 0 loss 0.69372:   8%|8         | 56/683 [00:42<05:33,  1.88it/s]epoch 0 loss 0.69383:   8%|8         | 56/683 [00:42<05:33,  1.88it/s]epoch 0 loss 0.69383:   8%|8         | 57/683 [00:42<05:31,  1.89it/s]epoch 0 loss 0.69393:   8%|8         | 57/683 [00:43<05:31,  1.89it/s]epoch 0 loss 0.69393:   8%|8         | 58/683 [00:43<05:29,  1.89it/s]epoch 0 loss 0.69389:   8%|8         | 58/683 [00:43<05:29,  1.89it/s]epoch 0 loss 0.69389:   9%|8         | 59/683 [00:43<05:28,  1.90it/s]epoch 0 loss 0.69363:   9%|8         | 59/683 [00:44<05:28,  1.90it/s]epoch 0 loss 0.69363:   9%|8         | 60/683 [00:44<05:29,  1.89it/s]epoch 0 loss 0.69338:   9%|8         | 60/683 [00:44<05:29,  1.89it/s]epoch 0 loss 0.69338:   9%|8         | 61/683 [00:44<05:27,  1.90it/s]epoch 0 loss 0.69349:   9%|8         | 61/683 [00:45<05:27,  1.90it/s]epoch 0 loss 0.69349:   9%|9         | 62/683 [00:45<05:27,  1.90it/s]epoch 0 loss 0.69287:   9%|9         | 62/683 [00:45<05:27,  1.90it/s]epoch 0 loss 0.69287:   9%|9         | 63/683 [00:46<05:27,  1.89it/s]epoch 0 loss 0.69314:   9%|9         | 63/683 [00:46<05:27,  1.89it/s]epoch 0 loss 0.69314:   9%|9         | 64/683 [00:46<05:26,  1.90it/s]epoch 0 loss 0.69305:   9%|9         | 64/683 [00:47<05:26,  1.90it/s]epoch 0 loss 0.69305:  10%|9         | 65/683 [00:47<05:26,  1.90it/s]epoch 0 loss 0.6931:  10%|9         | 65/683 [00:47<05:26,  1.90it/s] epoch 0 loss 0.6931:  10%|9         | 66/683 [00:47<05:30,  1.87it/s]epoch 0 loss 0.69254:  10%|9         | 66/683 [00:48<05:30,  1.87it/s]epoch 0 loss 0.69254:  10%|9         | 67/683 [00:48<05:29,  1.87it/s]epoch 0 loss 0.69241:  10%|9         | 67/683 [00:48<05:29,  1.87it/s]epoch 0 loss 0.69241:  10%|9         | 68/683 [00:48<05:29,  1.87it/s]epoch 0 loss 0.69202:  10%|9         | 68/683 [00:49<05:29,  1.87it/s]epoch 0 loss 0.69202:  10%|#         | 69/683 [00:49<05:26,  1.88it/s]epoch 0 loss 0.69229:  10%|#         | 69/683 [00:49<05:26,  1.88it/s]epoch 0 loss 0.69229:  10%|#         | 70/683 [00:49<05:25,  1.89it/s]epoch 0 loss 0.69249:  10%|#         | 70/683 [00:50<05:25,  1.89it/s]epoch 0 loss 0.69249:  10%|#         | 71/683 [00:50<05:24,  1.88it/s]epoch 0 loss 0.69215:  10%|#         | 71/683 [00:50<05:24,  1.88it/s]epoch 0 loss 0.69215:  11%|#         | 72/683 [00:50<05:23,  1.89it/s]epoch 0 loss 0.69235:  11%|#         | 72/683 [00:51<05:23,  1.89it/s]epoch 0 loss 0.69235:  11%|#         | 73/683 [00:51<05:21,  1.90it/s]epoch 0 loss 0.6919:  11%|#         | 73/683 [00:51<05:21,  1.90it/s] epoch 0 loss 0.6919:  11%|#         | 74/683 [00:51<05:21,  1.90it/s]epoch 0 loss 0.69198:  11%|#         | 74/683 [00:52<05:21,  1.90it/s]epoch 0 loss 0.69198:  11%|#         | 75/683 [00:52<05:20,  1.90it/s]epoch 0 loss 0.69196:  11%|#         | 75/683 [00:52<05:20,  1.90it/s]epoch 0 loss 0.69196:  11%|#1        | 76/683 [00:52<05:19,  1.90it/s]epoch 0 loss 0.69199:  11%|#1        | 76/683 [00:53<05:19,  1.90it/s]epoch 0 loss 0.69199:  11%|#1        | 77/683 [00:53<05:18,  1.90it/s]epoch 0 loss 0.69208:  11%|#1        | 77/683 [00:53<05:18,  1.90it/s]epoch 0 loss 0.69208:  11%|#1        | 78/683 [00:53<05:18,  1.90it/s]epoch 0 loss 0.69197:  11%|#1        | 78/683 [00:54<05:18,  1.90it/s]epoch 0 loss 0.69197:  12%|#1        | 79/683 [00:54<05:17,  1.90it/s]epoch 0 loss 0.69177:  12%|#1        | 79/683 [00:54<05:17,  1.90it/s]epoch 0 loss 0.69177:  12%|#1        | 80/683 [00:54<05:17,  1.90it/s]epoch 0 loss 0.69189:  12%|#1        | 80/683 [00:55<05:17,  1.90it/s]epoch 0 loss 0.69189:  12%|#1        | 81/683 [00:55<05:16,  1.90it/s]epoch 0 loss 0.69197:  12%|#1        | 81/683 [00:56<05:16,  1.90it/s]epoch 0 loss 0.69197:  12%|#2        | 82/683 [00:56<05:18,  1.89it/s]epoch 0 loss 0.69197:  12%|#2        | 82/683 [00:56<05:18,  1.89it/s]epoch 0 loss 0.69197:  12%|#2        | 83/683 [00:56<05:16,  1.89it/s]epoch 0 loss 0.69217:  12%|#2        | 83/683 [00:57<05:16,  1.89it/s]epoch 0 loss 0.69217:  12%|#2        | 84/683 [00:57<05:17,  1.89it/s]epoch 0 loss 0.69166:  12%|#2        | 84/683 [00:57<05:17,  1.89it/s]epoch 0 loss 0.69166:  12%|#2        | 85/683 [00:57<05:16,  1.89it/s]epoch 0 loss 0.69177:  12%|#2        | 85/683 [00:58<05:16,  1.89it/s]epoch 0 loss 0.69177:  13%|#2        | 86/683 [00:58<05:15,  1.89it/s]epoch 0 loss 0.69149:  13%|#2        | 86/683 [00:58<05:15,  1.89it/s]epoch 0 loss 0.69149:  13%|#2        | 87/683 [00:58<05:15,  1.89it/s]epoch 0 loss 0.69136:  13%|#2        | 87/683 [00:59<05:15,  1.89it/s]epoch 0 loss 0.69136:  13%|#2        | 88/683 [00:59<05:14,  1.89it/s]epoch 0 loss 0.69124:  13%|#2        | 88/683 [00:59<05:14,  1.89it/s]epoch 0 loss 0.69124:  13%|#3        | 89/683 [00:59<05:12,  1.90it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x000001B8A8244940>
Traceback (most recent call last):
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 1510, in __del__
    self._shutdown_workers()
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 1474, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\multiprocessing\process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 108, in wait
    res = _winapi.WaitForSingleObject(int(self._handle), msecs)
KeyboardInterrupt: 
epoch 0 loss 0.69124:  13%|#3        | 89/683 [01:00<06:44,  1.47it/s]
Traceback (most recent call last):
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 595, in <module>
    main()
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 566, in main
    train(args, train_dataset, model, tokenizer)
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 210, in train
    loss.backward()
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\site-packages\torch\_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\site-packages\torch\autograd\__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
^CTerminate batch job (Y/N)? 
^CTerminate batch job (Y/N)? 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py 	--output_folder_name=md 	--output_dir=./saved_models 	--model_type=roberta 	--tokenizer_name=microsoft/codebert-base 	--model_name_or_path=microsoft/codebert-base 	--do_test 	--train_data_file=../dataset/devign/train.jsonl 	--eval_data_file=../dataset/devign/valid.jsonl 	--test_data_file=../dataset/devign/test.jsonl 	--epoch 5 	--block_size 400 	--train_batch_size 32 	--eval_batch_size 64 	--learning_rate 2e-5 	--max_grad_norm 1.0 	--evaluate_during_training 	--seed 12 
^CTraceback (most recent call last):
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 34, in <module>
    import torch
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\site-packages\torch\__init__.py", line 124, in <module>
    res = kernel32.LoadLibraryExW(dll, None, 0x00001100)
KeyboardInterrupt
^CTerminate batch job (Y/N)? 
^C
(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py 	--output_folder_name=md 	--output_dir=./saved_models 	--model_type=roberta 	--tokenizer_name=microsoft/codebert-base 	--model_name_or_path=microsoft/codebert-base 	--do_train 	--train_data_file=../dataset/devign/train.jsonl 	--eval_data_file=../dataset/devign/valid.jsonl 	--test_data_file=../dataset/devign/test.jsonl 	--epoch 5 	--block_size 400 	--train_batch_size 32 	--eval_batch_size 64 	--learning_rate 2e-5 	--max_grad_norm 1.0 	--evaluate_during_training 	--seed 12 
08/28/2022 22:48:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/28/2022 22:48:20 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/devign/train.jsonl', output_folder_name='md', output_dir='./saved_models', eval_data_file='../dataset/devign/valid.jsonl', test_data_file='../dataset/devign/test.jsonl', model_type='roberta', model_name_or_path='microsoft/codebert-base', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/codebert-base', cache_dir='', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=12, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=32, per_gpu_eval_batch_size=64, start_epoch=0, start_step=0)
Traceback (most recent call last):
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 595, in <module>
    main()
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 562, in main
    train_dataset = TextDataset(tokenizer, args,args.train_data_file)
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 97, in __init__
    self.examples.append(convert_examples_to_features(js,tokenizer,args))
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 84, in convert_examples_to_features
    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\site-packages\transformers\tokenization_utils.py", line 547, in tokenize
    tokenized_text.extend(self._tokenize(token))
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\site-packages\transformers\models\roberta\tokenization_roberta.py", line 295, in _tokenize
    token = "".join(
KeyboardInterrupt
^CTerminate batch job (Y/N)? 
^CTerminate batch job (Y/N)? 
^CThe syntax of the command is incorrect.
(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>
(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM python run.py ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --output_folder_name=md ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --output_dir=./saved_models ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --model_type=roberta ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --tokenizer_name=microsoft/codebert-base ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --model_name_or_path=microsoft/codebert-base ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --do_train ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --train_data_file=../dataset/devign/train.jsonl ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --eval_data_file=../dataset/devign/valid.jsonl ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --test_data_file=../dataset/devign/test.jsonl ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --epoch 5 ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --block_size 400 ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --train_batch_size 32 ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --eval_batch_size 64 ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --learning_rate 2e-5 ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --max_grad_norm 1.0 ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --evaluate_during_training ^ 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>REM --seed 12 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py 	--output_folder_name=md 	--output_dir=./saved_models 	--model_type=roberta 	--tokenizer_name=microsoft/codebert-base 	--model_name_or_path=microsoft/codebert-base 	--do_test 	--train_data_file=../dataset/devign/train.jsonl 	--eval_data_file=../dataset/devign/valid.jsonl 	--test_data_file=../dataset/devign/test.jsonl 	--epoch 5 	--block_size 400 	--train_batch_size 32 	--eval_batch_size 64 	--learning_rate 2e-5 	--max_grad_norm 1.0 	--evaluate_during_training 	--seed 12 
08/28/2022 22:48:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/28/2022 22:48:34 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/devign/train.jsonl', output_folder_name='md', output_dir='./saved_models', eval_data_file='../dataset/devign/valid.jsonl', test_data_file='../dataset/devign/test.jsonl', model_type='roberta', model_name_or_path='microsoft/codebert-base', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/codebert-base', cache_dir='', block_size=400, do_train=False, do_eval=False, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=12, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=32, per_gpu_eval_batch_size=64, start_epoch=0, start_step=0)
08/28/2022 22:48:40 - INFO - __main__ -   ***** Running Test *****
08/28/2022 22:48:40 - INFO - __main__ -     Num examples = 2732
08/28/2022 22:48:40 - INFO - __main__ -     Batch size = 64
checkpoint-best-accmd/model.bin
  0%|          | 0/43 [00:00<?, ?it/s]  2%|2         | 1/43 [00:02<01:34,  2.25s/it]  5%|4         | 2/43 [00:02<00:46,  1.13s/it]  7%|6         | 3/43 [00:02<00:31,  1.28it/s]  9%|9         | 4/43 [00:03<00:24,  1.60it/s] 12%|#1        | 5/43 [00:03<00:20,  1.85it/s] 14%|#3        | 6/43 [00:04<00:17,  2.08it/s] 16%|#6        | 7/43 [00:04<00:16,  2.21it/s] 19%|#8        | 8/43 [00:04<00:14,  2.33it/s] 21%|##        | 9/43 [00:05<00:14,  2.39it/s] 23%|##3       | 10/43 [00:05<00:13,  2.48it/s] 26%|##5       | 11/43 [00:05<00:12,  2.57it/s] 28%|##7       | 12/43 [00:06<00:11,  2.63it/s] 30%|###       | 13/43 [00:06<00:11,  2.68it/s] 33%|###2      | 14/43 [00:07<00:10,  2.72it/s] 35%|###4      | 15/43 [00:07<00:10,  2.74it/s] 37%|###7      | 16/43 [00:07<00:09,  2.75it/s] 40%|###9      | 17/43 [00:08<00:09,  2.76it/s] 42%|####1     | 18/43 [00:08<00:09,  2.74it/s] 44%|####4     | 19/43 [00:08<00:08,  2.71it/s] 47%|####6     | 20/43 [00:09<00:08,  2.72it/s] 49%|####8     | 21/43 [00:09<00:08,  2.74it/s] 51%|#####1    | 22/43 [00:09<00:07,  2.76it/s] 53%|#####3    | 23/43 [00:10<00:07,  2.78it/s] 56%|#####5    | 24/43 [00:10<00:06,  2.79it/s] 58%|#####8    | 25/43 [00:11<00:06,  2.80it/s] 60%|######    | 26/43 [00:11<00:06,  2.80it/s] 63%|######2   | 27/43 [00:11<00:05,  2.80it/s] 65%|######5   | 28/43 [00:12<00:05,  2.80it/s] 67%|######7   | 29/43 [00:12<00:04,  2.80it/s] 70%|######9   | 30/43 [00:12<00:04,  2.80it/s] 72%|#######2  | 31/43 [00:13<00:04,  2.80it/s] 74%|#######4  | 32/43 [00:13<00:03,  2.80it/s] 77%|#######6  | 33/43 [00:13<00:03,  2.80it/s] 79%|#######9  | 34/43 [00:14<00:03,  2.80it/s] 81%|########1 | 35/43 [00:14<00:02,  2.79it/s] 84%|########3 | 36/43 [00:14<00:02,  2.80it/s] 86%|########6 | 37/43 [00:15<00:02,  2.80it/s] 88%|########8 | 38/43 [00:15<00:01,  2.80it/s] 91%|######### | 39/43 [00:16<00:01,  2.80it/s] 93%|#########3| 40/43 [00:16<00:01,  2.79it/s] 95%|#########5| 41/43 [00:16<00:00,  2.79it/s] 98%|#########7| 42/43 [00:17<00:00,  2.80it/s]100%|##########| 43/43 [00:17<00:00,  3.08it/s]100%|##########| 43/43 [00:17<00:00,  2.48it/s]

Accuracy: 0.6357979502196194
Precision: 0.6097972972972973
F-measure: 0.5920459204592046
Recall: 0.5752988047808765
