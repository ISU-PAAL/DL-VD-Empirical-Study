
(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_train --train_data_file=../dataset/balanced_0.8/train.jsonl --eval_data_file=../dataset/balanced_0.8/valid.jsonl --test_data_file=../dataset/balanced_0.8/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_eval --do_test --train_data_file=../dataset/balanced_0.8/train.jsonl --eval_data_file=../dataset/balanced_0.8/valid.jsonl --test_data_file=../dataset/balanced_0.8/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

Accuracy: 0.6442848219862586
Precision: 0.7569209993247805
F-measure: 0.6631174208813959
Recall: 0.59

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_train --train_data_file=../dataset/balanced_0.7/train.jsonl --eval_data_file=../dataset/balanced_0.7/valid.jsonl --test_data_file=../dataset/balanced_0.7/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_eval --do_test --train_data_file=../dataset/balanced_0.7/train.jsonl --eval_data_file=../dataset/balanced_0.7/valid.jsonl --test_data_file=../dataset/balanced_0.7/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

Accuracy: 0.6463590483056958
Precision: 0.737956204379562
F-measure: 0.6733266733266734
Recall: 0.6191059399877527

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_train --train_data_file=../dataset/balanced_0.6/train.jsonl --eval_data_file=../dataset/balanced_0.6/valid.jsonl --test_data_file=../dataset/balanced_0.6/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_eval --do_test --train_data_file=../dataset/balanced_0.6/train.jsonl --eval_data_file=../dataset/balanced_0.6/valid.jsonl --test_data_file=../dataset/balanced_0.6/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

Accuracy: 0.643581081081081
Precision: 0.6993217784476262
F-measure: 0.6874074074074074
Recall: 0.6758922068463219

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_train --train_data_file=../dataset/balanced_0.5/train.jsonl --eval_data_file=../dataset/balanced_0.5/valid.jsonl --test_data_file=../dataset/balanced_0.5/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_eval --do_test --train_data_file=../dataset/balanced_0.5/train.jsonl --eval_data_file=../dataset/balanced_0.5/valid.jsonl --test_data_file=../dataset/balanced_0.5/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

Accuracy: 0.6500508646998983
Precision: 0.6954624781849913
F-measure: 0.6985100788781771
Recall: 0.7015845070422535

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_train --train_data_file=../dataset/balanced_0.4/train.jsonl --eval_data_file=../dataset/balanced_0.4/valid.jsonl --test_data_file=../dataset/balanced_0.4/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_eval --do_test --train_data_file=../dataset/balanced_0.4/train.jsonl --eval_data_file=../dataset/balanced_0.4/valid.jsonl --test_data_file=../dataset/balanced_0.4/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

Accuracy: 0.6455223880597015
Precision: 0.7252604166666666
F-measure: 0.6615201900237528
Recall: 0.6080786026200873

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_train --train_data_file=../dataset/balanced_0.3/train.jsonl --eval_data_file=../dataset/balanced_0.3/valid.jsonl --test_data_file=../dataset/balanced_0.3/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_eval --do_test --train_data_file=../dataset/balanced_0.3/train.jsonl --eval_data_file=../dataset/balanced_0.3/valid.jsonl --test_data_file=../dataset/balanced_0.3/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

Accuracy: 0.6003330557868443
Precision: 0.7505938242280285
F-measure: 0.5683453237410071
Recall: 0.4573082489146165

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_train --train_data_file=../dataset/balanced_0.2/train.jsonl --eval_data_file=../dataset/balanced_0.2/valid.jsonl --test_data_file=../dataset/balanced_0.2/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_eval --do_test --train_data_file=../dataset/balanced_0.2/train.jsonl --eval_data_file=../dataset/balanced_0.2/valid.jsonl --test_data_file=../dataset/balanced_0.2/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

Accuracy: 0.605875152998776
Precision: 0.7724137931034483
F-measure: 0.5818181818181819
Recall: 0.4666666666666667

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_train --train_data_file=../dataset/balanced_0.1/train.jsonl --eval_data_file=../dataset/balanced_0.1/valid.jsonl --test_data_file=../dataset/balanced_0.1/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_eval --do_test --train_data_file=../dataset/balanced_0.1/train.jsonl --eval_data_file=../dataset/balanced_0.1/valid.jsonl --test_data_file=../dataset/balanced_0.1/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 

Accuracy: 0.5894206549118388
Precision: 0.6759259259259259
F-measure: 0.6417582417582417
Recall: 0.6108786610878661

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>python run.py --output_folder_name=balanced10 --output_dir=./saved_models --model_type=roberta --tokenizer_name=microsoft/codebert-base --model_name_or_path=microsoft/codebert-base --do_train --train_data_file=../dataset/balanced_1.0/train.jsonl --eval_data_file=../dataset/balanced_1.0/valid.jsonl --test_data_file=../dataset/balanced_1.0/test.jsonl --epoch 5 --block_size 400 --train_batch_size 32 --eval_batch_size 64 --learning_rate 2e-5 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 
08/19/2022 08:58:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/19/2022 08:58:57 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/balanced_1.0/train.jsonl', output_folder_name='balanced10', output_dir='./saved_models', eval_data_file='../dataset/balanced_1.0/valid.jsonl', test_data_file='../dataset/balanced_1.0/test.jsonl', model_type='roberta', model_name_or_path='microsoft/codebert-base', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/codebert-base', cache_dir='', block_size=400, do_train=True, do_eval=False, do_test=False, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=32, per_gpu_eval_batch_size=64, start_epoch=0, start_step=0)
08/19/2022 09:00:05 - INFO - __main__ -   *** Example ***
08/19/2022 09:00:05 - INFO - __main__ -   idx: 0
08/19/2022 09:00:05 - INFO - __main__ -   label: 0
08/19/2022 09:00:05 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_av', '_', 'cold', '_int', '_v', 'd', 'ade', 'c', '_', 'init', '(', 'AV', 'Cod', 'ec', 'Context', '_*', 'av', 'ctx', ')', '_{', '_V', 'D', 'AD', 'ec', 'oder', 'Context', '_*', 'ctx', '_=', '_av', 'ctx', '->', 'priv', '_', 'data', ';', '_struct', '_v', 'da', '_', 'context', '_*', 'v', 'da', '_', 'ctx', '_=', '_&', 'ctx', '->', 'v', 'da', '_', 'ctx', ';', '_OS', 'Status', '_status', ';', '_int', '_ret', ';', '_c', 'tx', '->', 'h', '264', '_', 'initialized', '_=', '_0', ';', '_/*', '_init', '_p', 'ix', '_', 'fm', 'ts', '_of', '_codec', '_*/', '_if', '_(!', 'ff', '_', 'h', '264', '_', 'v', 'da', '_', 'dec', 'oder', '.', 'p', 'ix', '_', 'fm', 'ts', ')', '_{', '_if', '_(', 'k', 'C', 'FC', 'ore', 'Found', 'ation', 'Version', 'Number', '_<', '_k', 'C', 'FC', 'ore', 'Found', 'ation', 'Version', 'Number', '10', '_', '7', ')', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'dec', 'oder', '.', 'p', 'ix', '_', 'fm', 'ts', '_=', '_v', 'da', '_', 'p', 'ix', 'fm', 'ts', '_', 'pri', 'or', '_', '10', '_', '7', ';', '_else', '_ff', '_', 'h', '264', '_', 'v', 'da', '_', 'dec', 'oder', '.', 'p', 'ix', '_', 'fm', 'ts', '_=', '_v', 'da', '_', 'p', 'ix', 'fm', 'ts', ';', '_}', '_/*', '_init', '_v', 'da', '_*/', '_mem', 'set', '(', 'v', 'da', '_', 'ctx', ',', '_0', ',', '_sizeof', '(', 'struct', '_v', 'da', '_', 'context', '));', '_v', 'da', '_', 'ctx', '->', 'width', '_=', '_av', 'ctx', '->', 'width', ';', '_v', 'da', '_', 'ctx', '->', 'height', '_=', '_av', 'ctx', '->', 'height', ';', '_v', 'da', '_', 'ctx', '->', 'format', '_=', "_'", 'av', 'c', '1', "';", '_v', 'da', '_', 'ctx', '->', 'use', '_', 'sync', '_', 'dec', 'oding', '_=', '_1', ';', '_v', 'da', '_', 'ctx', '->', 'use', '_', 'ref', '_', 'buffer', '_=', '_1', ';', '_c', 'tx', '->', 'p', 'ix', '_', 'f', 'mt', '_=', '_av', 'ctx', '->', 'get', '_', 'format', '(', 'av', 'ctx', ',', '_av', 'ctx', '->', 'cod', 'ec', '->', 'p', 'ix', '_', 'fm', 'ts', ');', '_switch', '_(', 'ctx', '->', 'p', 'ix', '_', 'f', 'mt', ')', '_{', '_case', '_AV', '_', 'P', 'IX', '_', 'F', 'MT', '_', 'U', 'Y', 'V', 'Y', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'p', 'ix', '_', 'f', 'mt', '_', 'type', '_=', "_'", '2', 'v', 'uy', "';", '_break', ';', '_case', '_AV', '_', 'P', 'IX', '_', 'F', 'MT', '_', 'Y', 'U', 'Y', 'V', '422', ':', '_v', 'da', '_', 'ctx', '->', 'cv', '_', 'p', 'ix', '_', 'f', 'mt', '_', 'type', '_=', "_'", 'yu', 'vs', "';", '_break', ';', '_case', '_AV', '_', 'P', 'IX', '_', 'F', 'MT', '_', 'NV', '12', ':', '</s>']
08/19/2022 09:00:05 - INFO - __main__ -   input_ids: 0 42653 6402 1215 33912 6979 748 417 1829 438 1215 25153 1640 10612 47436 3204 48522 1009 1469 49575 43 25522 468 495 2606 3204 15362 48522 1009 49575 5457 6402 49575 46613 25943 1215 23687 131 29916 748 6106 1215 46796 1009 705 6106 1215 49575 5457 359 49575 46613 705 6106 1215 49575 131 8192 47731 2194 131 6979 5494 131 740 43820 46613 298 29137 1215 49722 5457 321 131 48565 45511 181 3181 1215 40523 1872 9 45797 48404 114 48209 3145 1215 298 29137 1215 705 6106 1215 11127 15362 4 642 3181 1215 40523 1872 43 25522 114 36 330 347 5268 1688 29991 1258 47322 43623 28696 449 347 5268 1688 29991 1258 47322 43623 698 1215 406 43 48400 1215 298 29137 1215 705 6106 1215 11127 15362 4 642 3181 1215 40523 1872 5457 748 6106 1215 642 3181 40523 1872 1215 13718 368 1215 698 1215 406 131 1493 48400 1215 298 29137 1215 705 6106 1215 11127 15362 4 642 3181 1215 40523 1872 5457 748 6106 1215 642 3181 40523 1872 131 35524 48565 45511 748 6106 48404 26012 8738 1640 705 6106 1215 49575 6 321 6 49907 1640 25384 748 6106 1215 46796 48749 748 6106 1215 49575 46613 36097 5457 6402 49575 46613 36097 131 748 6106 1215 49575 46613 37009 5457 6402 49575 46613 37009 131 748 6106 1215 49575 46613 34609 5457 128 1469 438 134 23500 748 6106 1215 49575 46613 3698 1215 45176 1215 11127 19519 5457 112 131 748 6106 1215 49575 46613 3698 1215 13043 1215 47438 5457 112 131 740 43820 46613 642 3181 1215 506 16100 5457 6402 49575 46613 6460 1215 34609 1640 1469 49575 6 6402 49575 46613 29659 3204 46613 642 3181 1215 40523 1872 4397 5405 36 49575 46613 642 3181 1215 506 16100 43 25522 403 17307 1215 510 9482 1215 597 11674 1215 791 975 846 975 37319 35 748 6106 1215 49575 46613 38635 1215 642 3181 1215 506 16100 1215 12528 5457 128 176 705 5781 23500 1108 131 403 17307 1215 510 9482 1215 597 11674 1215 975 791 975 846 37319 35 748 6106 1215 49575 46613 38635 1215 642 3181 1215 506 16100 1215 12528 5457 128 29159 15597 23500 1108 131 403 17307 1215 510 9482 1215 597 11674 1215 31668 1092 35 2
08/19/2022 09:00:05 - INFO - __main__ -   *** Example ***
08/19/2022 09:00:05 - INFO - __main__ -   idx: 1
08/19/2022 09:00:05 - INFO - __main__ -   label: 0
08/19/2022 09:00:05 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_void', '_v', '4', 'l', '2', '_', 'free', '_', 'buffer', '(', 'void', '_*', 'op', 'aque', ',', '_uint', '8', '_', 't', '_*', 'un', 'used', ')', '_{', '_V', '4', 'L', '2', 'Buffer', '*', '_av', 'buf', '_=', '_opaque', ';', '_V', '4', 'L', '2', 'm', '2', 'm', 'Context', '_*', 's', '_=', '_buf', '_', 'to', '_', 'm', '2', 'm', 'ctx', '(', 'av', 'buf', ');', '_if', '_(', 'atomic', '_', 'f', 'etch', '_', 'sub', '(&', 'av', 'buf', '->', 'context', '_', 'ref', 'count', ',', '_1', ')', '_==', '_1', ')', '_{', '_atomic', '_', 'f', 'etch', '_', 'sub', '_', 'expl', 'icit', '(&', 's', '->', 'ref', 'count', ',', '_1', ',', '_memory', '_', 'order', '_', 'ac', 'q', '_', 'rel', ');', '_if', '_(', 's', '->', 're', 'init', ')', '_{', '_if', '_(!', 'atomic', '_', 'load', '(&', 's', '->', 'ref', 'count', '))', '_sem', '_', 'post', '(&', 's', '->', 'ref', 'sync', ');', '_}', '_else', '_if', '_(', 'av', 'buf', '->', 'context', '->', 'stream', 'on', ')', '_ff', '_', 'v', '4', 'l', '2', '_', 'buffer', '_', 'en', 'queue', '(', 'av', 'buf', ');', '_av', '_', 'buffer', '_', 'un', 'ref', '(&', 'av', 'buf', '->', 'context', '_', 'ref', ');', '_}', '_}', '</s>']
08/19/2022 09:00:05 - INFO - __main__ -   input_ids: 0 42653 13842 748 306 462 176 1215 3743 1215 47438 1640 47908 1009 1517 35485 6 49315 398 1215 90 1009 879 6199 43 25522 468 306 574 176 49334 3226 6402 48939 5457 31861 131 468 306 574 176 119 176 119 48522 1009 29 5457 49125 1215 560 1215 119 176 119 49575 1640 1469 48939 4397 114 36 45826 1215 506 29094 1215 10936 49763 1469 48939 46613 46796 1215 13043 11432 6 112 43 45994 112 43 25522 21495 1215 506 29094 1215 10936 1215 23242 17022 49763 29 46613 13043 11432 6 112 6 3783 1215 10337 1215 1043 1343 1215 5982 4397 114 36 29 46613 241 25153 43 25522 114 48209 45826 1215 16204 49763 29 46613 13043 11432 35122 9031 1215 7049 49763 29 46613 13043 45176 4397 35524 1493 114 36 1469 48939 46613 46796 46613 8656 261 43 48400 1215 705 306 462 176 1215 47438 1215 225 48702 1640 1469 48939 4397 6402 1215 47438 1215 879 13043 49763 1469 48939 46613 46796 1215 13043 4397 35524 35524 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
08/19/2022 09:00:05 - INFO - __main__ -   *** Example ***
08/19/2022 09:00:05 - INFO - __main__ -   idx: 2
08/19/2022 09:00:05 - INFO - __main__ -   label: 0
08/19/2022 09:00:05 - INFO - __main__ -   input_tokens: ['<s>', 'int', '_ff', '_', 'get', '_', 'wav', '_', 'header', '(', 'AV', 'Format', 'Context', '_*', 's', ',', '_AV', 'IO', 'Context', '_*', 'pb', ',', '_AV', 'Cod', 'ec', 'Context', '_*', 'cod', 'ec', ',', '_int', '_size', ',', '_int', '_big', '_', 'end', 'ian', ')', '_{', '_int', '_id', ';', '_uint', '64', '_', 't', '_bit', 'rate', ';', '_if', '_(', 'size', '_<', '_14', ')', '_{', '_av', 'priv', '_', 'request', '_', 'sample', '(', 'cod', 'ec', ',', '_"', 'wav', '_header', '_size', '_<', '_14', '");', '_return', '_A', 'VER', 'ROR', '_', 'IN', 'VAL', 'ID', 'DATA', ';', '_}', '_codec', '->', 'cod', 'ec', '_', 'type', '_=', '_AV', 'MED', 'IA', '_', 'TYPE', '_', 'AUD', 'IO', ';', '_if', '_(!', 'big', '_', 'end', 'ian', ')', '_{', '_id', '_=', '_av', 'io', '_', 'rl', '16', '(', 'pb', ');', '_if', '_(', 'id', '_!=', '_0', 'x', '01', '65', ')', '_{', '_codec', '->', 'ch', 'annels', '_=', '_av', 'io', '_', 'rl', '16', '(', 'pb', ');', '_codec', '->', 'sample', '_', 'rate', '_=', '_av', 'io', '_', 'rl', '32', '(', 'pb', ');', '_bit', 'rate', '_=', '_av', 'io', '_', 'rl', '32', '(', 'pb', ')', '_*', '_8', 'LL', ';', '_codec', '->', 'block', '_', 'align', '_=', '_av', 'io', '_', 'rl', '16', '(', 'pb', ');', '_}', '_}', '_else', '_{', '_id', '_=', '_av', 'io', '_', 'rb', '16', '(', 'pb', ');', '_codec', '->', 'ch', 'annels', '_=', '_av', 'io', '_', 'rb', '16', '(', 'pb', ');', '_codec', '->', 'sample', '_', 'rate', '_=', '_av', 'io', '_', 'rb', '32', '(', 'pb', ');', '_bit', 'rate', '_=', '_av', 'io', '_', 'rb', '32', '(', 'pb', ')', '_*', '_8', 'LL', ';', '_codec', '->', 'block', '_', 'align', '_=', '_av', 'io', '_', 'rb', '16', '(', 'pb', ');', '_}', '_if', '_(', 'size', '_==', '_14', ')', '_{', '_/*', '_We', "'re", '_dealing', '_with', '_plain', '_vanilla', '_WA', 'VE', 'FORM', 'AT', '_*/', '_codec', '->', 'bits', '_', 'per', '_', 'coded', '_', 'sample', '_=', '_8', ';', '_}', '_else', '_{', '_if', '_(!', 'big', '_', 'end', 'ian', ')', '_{', '_codec', '->', 'bits', '_', 'per', '_', 'coded', '_', 'sample', '_=', '_av', 'io', '_', 'rl', '16', '(', 'pb', ');', '_}', '_else', '_{', '_codec', '->', 'bits', '_', 'per', '_', 'coded', '_', 'sample', '_=', '_av', 'io', '_', 'rb', '16', '(', 'pb', ');', '_}', '_}', '_if', '_(', 'id', '_==', '_0', 'x', 'FF', 'FE', ')', '_{', '_codec', '->', 'cod', 'ec', '_', 'tag', '_=', '_0', ';', '_}', '_else', '_{', '_codec', '->', 'cod', 'ec', '_', 'tag', '_=', '_id', ';', '_codec', '->', 'cod', 'ec', '_', 'id', '_=', '_ff', '_', 'wav', '_', 'cod', 'ec', '_', 'get', '_', 'id', '(', 'id', ',', '_codec', '->', 'bits', '_', 'per', '_', 'coded', '_', 'sample', '</s>']
08/19/2022 09:00:05 - INFO - __main__ -   input_ids: 0 2544 48400 1215 6460 1215 48479 1215 24419 1640 10612 48587 48522 1009 29 6 17307 6454 48522 1009 45306 6 17307 47436 3204 48522 1009 29659 3204 6 6979 1836 6 6979 380 1215 1397 811 43 25522 6979 13561 131 49315 4027 1215 90 828 7954 131 114 36 10799 28696 501 43 25522 6402 25943 1215 16604 1215 14029 1640 29659 3204 6 22 48479 12734 1836 28696 501 45751 671 83 9847 45055 1215 2444 39766 2688 48242 131 35524 45797 46613 29659 3204 1215 12528 5457 17307 32653 2889 1215 48710 1215 44741 6454 131 114 48209 8527 1215 1397 811 43 25522 13561 5457 6402 1020 1215 41857 1549 1640 45306 4397 114 36 808 49333 321 1178 2663 3506 43 25522 45797 46613 611 34735 5457 6402 1020 1215 41857 1549 1640 45306 4397 45797 46613 14029 1215 7954 5457 6402 1020 1215 41857 2881 1640 45306 4397 828 7954 5457 6402 1020 1215 41857 2881 1640 45306 43 1009 290 6006 131 45797 46613 16776 1215 44022 5457 6402 1020 1215 41857 1549 1640 45306 4397 35524 35524 1493 25522 13561 5457 6402 1020 1215 20815 1549 1640 45306 4397 45797 46613 611 34735 5457 6402 1020 1215 20815 1549 1640 45306 4397 45797 46613 14029 1215 7954 5457 6402 1020 1215 20815 2881 1640 45306 4397 828 7954 5457 6402 1020 1215 20815 2881 1640 45306 43 1009 290 6006 131 45797 46613 16776 1215 44022 5457 6402 1020 1215 20815 1549 1640 45306 4397 35524 114 36 10799 45994 501 43 25522 48565 166 214 4098 19 10798 21857 9342 8856 38036 2571 48404 45797 46613 32131 1215 1741 1215 43301 1215 14029 5457 290 131 35524 1493 25522 114 48209 8527 1215 1397 811 43 25522 45797 46613 32131 1215 1741 1215 43301 1215 14029 5457 6402 1020 1215 41857 1549 1640 45306 4397 35524 1493 25522 45797 46613 32131 1215 1741 1215 43301 1215 14029 5457 6402 1020 1215 20815 1549 1640 45306 4397 35524 35524 114 36 808 45994 321 1178 7389 10885 43 25522 45797 46613 29659 3204 1215 10058 5457 321 131 35524 1493 25522 45797 46613 29659 3204 1215 10058 5457 13561 131 45797 46613 29659 3204 1215 808 5457 48400 1215 48479 1215 29659 3204 1215 6460 1215 808 1640 808 6 45797 46613 32131 1215 1741 1215 43301 1215 14029 2
C:\Users\<ANONYMOUS>\Anaconda3\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
08/19/2022 09:00:05 - INFO - __main__ -   ***** Running training *****
08/19/2022 09:00:05 - INFO - __main__ -     Num examples = 41267
08/19/2022 09:00:05 - INFO - __main__ -     Num Epochs = 5
08/19/2022 09:00:05 - INFO - __main__ -     Instantaneous batch size per GPU = 32
08/19/2022 09:00:05 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
08/19/2022 09:00:05 - INFO - __main__ -     Gradient Accumulation steps = 1
08/19/2022 09:00:05 - INFO - __main__ -     Total optimization steps = 6450
  0%|          | 0/1290 [00:00<?, ?it/s]epoch 0 loss 0.69832:   0%|          | 0/1290 [00:16<?, ?it/s]epoch 0 loss 0.69832:   0%|          | 1/1290 [00:16<5:53:34, 16.46s/it]epoch 0 loss 0.68784:   0%|          | 1/1290 [00:16<5:53:34, 16.46s/it]epoch 0 loss 0.68784:   0%|          | 2/1290 [00:16<2:31:55,  7.08s/it]epoch 0 loss 0.68293:   0%|          | 2/1290 [00:17<2:31:55,  7.08s/it]epoch 0 loss 0.68293:   0%|          | 3/1290 [00:17<1:27:28,  4.08s/it]epoch 0 loss 0.68519:   0%|          | 3/1290 [00:17<1:27:28,  4.08s/it]epoch 0 loss 0.68519:   0%|          | 4/1290 [00:17<57:11,  2.67s/it]  epoch 0 loss 0.68623:   0%|          | 4/1290 [00:18<57:11,  2.67s/it]epoch 0 loss 0.68623:   0%|          | 5/1290 [00:18<40:28,  1.89s/it]epoch 0 loss 0.68401:   0%|          | 5/1290 [00:18<40:28,  1.89s/it]epoch 0 loss 0.68401:   0%|          | 6/1290 [00:19<30:24,  1.42s/it]epoch 0 loss 0.68744:   0%|          | 6/1290 [00:19<30:24,  1.42s/it]epoch 0 loss 0.68744:   1%|          | 7/1290 [00:19<24:01,  1.12s/it]epoch 0 loss 0.68594:   1%|          | 7/1290 [00:20<24:01,  1.12s/it]epoch 0 loss 0.68594:   1%|          | 8/1290 [00:20<19:50,  1.08it/s]epoch 0 loss 0.6879:   1%|          | 8/1290 [00:20<19:50,  1.08it/s] epoch 0 loss 0.6879:   1%|          | 9/1290 [00:20<17:01,  1.25it/s]epoch 0 loss 0.69021:   1%|          | 9/1290 [00:21<17:01,  1.25it/s]epoch 0 loss 0.69021:   1%|          | 10/1290 [00:21<15:08,  1.41it/s]epoch 0 loss 0.69018:   1%|          | 10/1290 [00:21<15:08,  1.41it/s]epoch 0 loss 0.69018:   1%|          | 11/1290 [00:21<13:49,  1.54it/s]epoch 0 loss 0.68952:   1%|          | 11/1290 [00:22<13:49,  1.54it/s]epoch 0 loss 0.68952:   1%|          | 12/1290 [00:22<12:55,  1.65it/s]epoch 0 loss 0.68966:   1%|          | 12/1290 [00:22<12:55,  1.65it/s]epoch 0 loss 0.68966:   1%|1         | 13/1290 [00:22<12:18,  1.73it/s]epoch 0 loss 0.6914:   1%|1         | 13/1290 [00:23<12:18,  1.73it/s] epoch 0 loss 0.6914:   1%|1         | 14/1290 [00:23<11:52,  1.79it/s]epoch 0 loss 0.69145:   1%|1         | 14/1290 [00:23<11:52,  1.79it/s]epoch 0 loss 0.69145:   1%|1         | 15/1290 [00:23<11:33,  1.84it/s]epoch 0 loss 0.69227:   1%|1         | 15/1290 [00:24<11:33,  1.84it/s]epoch 0 loss 0.69227:   1%|1         | 16/1290 [00:24<11:21,  1.87it/s]epoch 0 loss 0.69142:   1%|1         | 16/1290 [00:24<11:21,  1.87it/s]epoch 0 loss 0.69142:   1%|1         | 17/1290 [00:24<11:12,  1.89it/s]epoch 0 loss 0.69086:   1%|1         | 17/1290 [00:25<11:12,  1.89it/s]epoch 0 loss 0.69086:   1%|1         | 18/1290 [00:25<11:05,  1.91it/s]epoch 0 loss 0.68986:   1%|1         | 18/1290 [00:25<11:05,  1.91it/s]epoch 0 loss 0.68986:   1%|1         | 19/1290 [00:25<11:01,  1.92it/s]epoch 0 loss 0.68882:   1%|1         | 19/1290 [00:26<11:01,  1.92it/s]epoch 0 loss 0.68882:   2%|1         | 20/1290 [00:26<10:59,  1.93it/s]epoch 0 loss 0.68909:   2%|1         | 20/1290 [00:26<10:59,  1.93it/s]epoch 0 loss 0.68909:   2%|1         | 21/1290 [00:26<10:57,  1.93it/s]epoch 0 loss 0.68755:   2%|1         | 21/1290 [00:27<10:57,  1.93it/s]epoch 0 loss 0.68755:   2%|1         | 22/1290 [00:27<10:55,  1.94it/s]epoch 0 loss 0.68802:   2%|1         | 22/1290 [00:27<10:55,  1.94it/s]epoch 0 loss 0.68802:   2%|1         | 23/1290 [00:27<10:53,  1.94it/s]epoch 0 loss 0.68811:   2%|1         | 23/1290 [00:28<10:53,  1.94it/s]epoch 0 loss 0.68811:   2%|1         | 24/1290 [00:28<10:53,  1.94it/s]epoch 0 loss 0.68921:   2%|1         | 24/1290 [00:28<10:53,  1.94it/s]epoch 0 loss 0.68921:   2%|1         | 25/1290 [00:28<10:53,  1.93it/s]epoch 0 loss 0.6892:   2%|1         | 25/1290 [00:29<10:53,  1.93it/s] epoch 0 loss 0.6892:   2%|2         | 26/1290 [00:29<10:54,  1.93it/s]epoch 0 loss 0.6895:   2%|2         | 26/1290 [00:29<10:54,  1.93it/s]epoch 0 loss 0.6895:   2%|2         | 27/1290 [00:29<10:55,  1.93it/s]epoch 0 loss 0.68974:   2%|2         | 27/1290 [00:30<10:55,  1.93it/s]epoch 0 loss 0.68974:   2%|2         | 28/1290 [00:30<10:55,  1.92it/s]epoch 0 loss 0.68998:   2%|2         | 28/1290 [00:30<10:55,  1.92it/s]epoch 0 loss 0.68998:   2%|2         | 29/1290 [00:30<10:58,  1.91it/s]epoch 0 loss 0.6898:   2%|2         | 29/1290 [00:31<10:58,  1.91it/s] epoch 0 loss 0.6898:   2%|2         | 30/1290 [00:31<10:56,  1.92it/s]epoch 0 loss 0.69023:   2%|2         | 30/1290 [00:31<10:56,  1.92it/s]epoch 0 loss 0.69023:   2%|2         | 31/1290 [00:31<10:54,  1.92it/s]epoch 0 loss 0.69037:   2%|2         | 31/1290 [00:32<10:54,  1.92it/s]epoch 0 loss 0.69037:   2%|2         | 32/1290 [00:32<10:57,  1.91it/s]epoch 0 loss 0.69043:   2%|2         | 32/1290 [00:32<10:57,  1.91it/s]epoch 0 loss 0.69043:   3%|2         | 33/1290 [00:32<10:56,  1.91it/s]epoch 0 loss 0.69009:   3%|2         | 33/1290 [00:33<10:56,  1.91it/s]epoch 0 loss 0.69009:   3%|2         | 34/1290 [00:33<10:59,  1.91it/s]epoch 0 loss 0.69091:   3%|2         | 34/1290 [00:33<10:59,  1.91it/s]epoch 0 loss 0.69091:   3%|2         | 35/1290 [00:33<10:57,  1.91it/s]epoch 0 loss 0.69083:   3%|2         | 35/1290 [00:34<10:57,  1.91it/s]epoch 0 loss 0.69083:   3%|2         | 36/1290 [00:34<10:53,  1.92it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x000001E16CE02820>
Traceback (most recent call last):
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 1510, in __del__
    self._shutdown_workers()
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py", line 1498, in _shutdown_workers
    w.terminate()
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\multiprocessing\process.py", line 133, in terminate
    self._popen.terminate()
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 125, in terminate
    if self.wait(timeout=1.0) is None:
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\multiprocessing\popen_spawn_win32.py", line 108, in wait
    res = _winapi.WaitForSingleObject(int(self._handle), msecs)
KeyboardInterrupt: 
epoch 0 loss 0.69083:   3%|2         | 36/1290 [00:35<20:24,  1.02it/s]
Traceback (most recent call last):
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 585, in <module>
    main()
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 558, in main
    train(args, train_dataset, model, tokenizer)
  File "C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code\run.py", line 210, in train
    loss.backward()
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\site-packages\torch\_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\<ANONYMOUS>\Anaconda3\lib\site-packages\torch\autograd\__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
^CTerminate batch job (Y/N)? 
^CThe syntax of the command is incorrect.

(base) C:\Users\<ANONYMOUS>\Documents\CodeXGLUE-main\Code-Code\Defect-detection\code>