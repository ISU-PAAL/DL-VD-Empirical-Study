+ val=non
+ python Finetuning+evaluation_VulBERTa-CNN_diverse.py --seed_input=10102 --data_folder=non --train_test=train --batch=3 --epochs=10 --dataset=combined --generate_json=False
Device:  cuda
MultiGPU:  False
data/finetune/combined/nondiverse/train.jsonl
data/finetune/combined/nondiverse/valid.jsonl
Traceback (most recent call last):
  File "Finetuning+evaluation_VulBERTa-CNN_diverse.py", line 492, in <module>
    m3.func = m3.func.apply(cleaner)
NameError: name 'm3' is not defined
+ val=non
+ python Finetuning+evaluation_VulBERTa-CNN_diverse.py --seed_input=10102 --data_folder=non --train_test=train --batch=3 --epochs=10 --dataset=combined --generate_json=False
Device:  cuda
MultiGPU:  False
data/finetune/combined/nondiverse/train.jsonl
data/finetune/combined/nondiverse/valid.jsonl
Traceback (most recent call last):
  File "Finetuning+evaluation_VulBERTa-CNN_diverse.py", line 556, in <module>
    test_data = TabularDataset_From_List(test_encodings,'dict',fields = fields)
NameError: name 'test_encodings' is not defined
+ val=non
+ python Finetuning+evaluation_VulBERTa-CNN_diverse.py --seed_input=10102 --data_folder=non --train_test=train --batch=3 --epochs=10 --dataset=combined --generate_json=False
Device:  cuda
MultiGPU:  False
data/finetune/combined/nondiverse/train.jsonl
data/finetune/combined/nondiverse/valid.jsonl

Traceback (most recent call last):
  File "Finetuning+evaluation_VulBERTa-CNN_diverse.py", line 631, in <module>
    (train_data, val_data, test_data, holdout_data),
NameError: name 'test_data' is not defined
+ val=non
+ python Finetuning+evaluation_VulBERTa-CNN_diverse.py --seed_input=10102 --data_folder=non --train_test=train --batch=3 --epochs=10 --dataset=combined --generate_json=False
Some weights of RobertaModel were not initialized from the model checkpoint at models/pre_trained/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Device:  cuda
MultiGPU:  False
data/finetune/combined/nondiverse/train.jsonl
data/finetune/combined/nondiverse/valid.jsonl

myCNN(
  (embed): Embedding(50000, 768, padding_idx=1)
  (conv1): Conv1d(768, 200, kernel_size=(3,), stride=(1,))
  (conv2): Conv1d(768, 200, kernel_size=(4,), stride=(1,))
  (conv3): Conv1d(768, 200, kernel_size=(5,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc1): Linear(in_features=600, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=2, bias=True)
)
Num of trainable param:  2030810
Training started.....
Epoch 1 - Training acc: 0.7500 -Training loss: 0.6830 - Val acc: 0.3333 - Val loss: 0.7141 - Time: 0.0486s 
Epoch 2 - Training acc: 0.7500 -Training loss: 0.6624 - Val acc: 0.3333 - Val loss: 0.7238 - Time: 0.0460s 
Epoch 3 - Training acc: 0.7500 -Training loss: 0.6490 - Val acc: 0.3333 - Val loss: 0.7331 - Time: 0.0461s 
Epoch 4 - Training acc: 0.7500 -Training loss: 0.6326 - Val acc: 0.3333 - Val loss: 0.7463 - Time: 0.0466s 
Epoch 5 - Training acc: 0.7500 -Training loss: 0.6095 - Val acc: 0.3333 - Val loss: 0.7685 - Time: 0.0461s 
Epoch 6 - Training acc: 0.7500 -Training loss: 0.5851 - Val acc: 0.3333 - Val loss: 0.8012 - Time: 0.0477s 
Epoch 7 - Training acc: 0.7500 -Training loss: 0.5355 - Val acc: 0.3333 - Val loss: 0.8458 - Time: 0.0466s 
Epoch 8 - Training acc: 0.7500 -Training loss: 0.4961 - Val acc: 0.3333 - Val loss: 0.9107 - Time: 0.0468s 
Epoch 9 - Training acc: 0.7500 -Training loss: 0.4360 - Val acc: 0.3333 - Val loss: 0.9932 - Time: 0.0461s 
Epoch 10 - Training acc: 0.7500 -Training loss: 0.4089 - Val acc: 0.3333 - Val loss: 1.0452 - Time: 0.0459s 
Training completed!
+ val=non
+ python Finetuning+evaluation_VulBERTa-CNN_diverse.py --seed_input=10102 --data_folder=non --train_test=train --batch=3 --epochs=10 --dataset=combined --generate_json=False
Some weights of RobertaModel were not initialized from the model checkpoint at models/pre_trained/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Device:  cuda
MultiGPU:  False
data/finetune/combined/nondiverse/train.jsonl
data/finetune/combined/nondiverse/valid.jsonl

myCNN(
  (embed): Embedding(50000, 768, padding_idx=1)
  (conv1): Conv1d(768, 200, kernel_size=(3,), stride=(1,))
  (conv2): Conv1d(768, 200, kernel_size=(4,), stride=(1,))
  (conv3): Conv1d(768, 200, kernel_size=(5,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc1): Linear(in_features=600, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=2, bias=True)
)
Num of trainable param:  2030810
Training started.....
Epoch 1 - Training acc: 0.7500 -Training loss: 0.6830 - Val acc: 0.3333 - Val loss: 0.7141 - Time: 0.0604s 
Epoch 2 - Training acc: 0.7500 -Training loss: 0.6624 - Val acc: 0.3333 - Val loss: 0.7238 - Time: 0.0441s 
Epoch 3 - Training acc: 0.7500 -Training loss: 0.6490 - Val acc: 0.3333 - Val loss: 0.7331 - Time: 0.0459s 
Epoch 4 - Training acc: 0.7500 -Training loss: 0.6326 - Val acc: 0.3333 - Val loss: 0.7463 - Time: 0.0443s 
Epoch 5 - Training acc: 0.7500 -Training loss: 0.6095 - Val acc: 0.3333 - Val loss: 0.7685 - Time: 0.0497s 
Epoch 6 - Training acc: 0.7500 -Training loss: 0.5851 - Val acc: 0.3333 - Val loss: 0.8012 - Time: 0.0429s 
Epoch 7 - Training acc: 0.7500 -Training loss: 0.5355 - Val acc: 0.3333 - Val loss: 0.8458 - Time: 0.0451s 
Epoch 8 - Training acc: 0.7500 -Training loss: 0.4961 - Val acc: 0.3333 - Val loss: 0.9107 - Time: 0.0449s 
Epoch 9 - Training acc: 0.7500 -Training loss: 0.4360 - Val acc: 0.3333 - Val loss: 0.9932 - Time: 0.0435s 
Epoch 10 - Training acc: 0.7500 -Training loss: 0.4089 - Val acc: 0.3333 - Val loss: 1.0452 - Time: 0.0425s 
Training completed!
+ val=non
+ python Finetuning+evaluation_VulBERTa-CNN_diverse.py --seed_input=10102 --data_folder=non --train_test=train --batch=16 --epochs=10 --dataset=combined --generate_json=False
Some weights of RobertaModel were not initialized from the model checkpoint at models/pre_trained/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Device:  cuda
MultiGPU:  False
data/finetune/combined/nondiverse/train.jsonl
data/finetune/combined/nondiverse/valid.jsonl

myCNN(
  (embed): Embedding(50000, 768, padding_idx=1)
  (conv1): Conv1d(768, 200, kernel_size=(3,), stride=(1,))
  (conv2): Conv1d(768, 200, kernel_size=(4,), stride=(1,))
  (conv3): Conv1d(768, 200, kernel_size=(5,), stride=(1,))
  (dropout): Dropout(p=0.5, inplace=False)
  (fc1): Linear(in_features=600, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=2, bias=True)
)
Num of trainable param:  2030810
Training started.....
Epoch 1 - Training acc: 0.9983 -Training loss: 0.0154 - Val acc: 0.0538 - Val loss: 54.1089 - Time: 119.8358s 
Epoch 2 - Training acc: 0.9935 -Training loss: 0.1077 - Val acc: 0.0538 - Val loss: 13.9418 - Time: 115.3718s 
Epoch 3 - Training acc: 0.9883 -Training loss: 0.0881 - Val acc: 0.0538 - Val loss: 15.7847 - Time: 115.8384s 
Epoch 4 - Training acc: 0.9891 -Training loss: 0.0911 - Val acc: 0.0538 - Val loss: 17.9191 - Time: 115.5916s 
Epoch 5 - Training acc: 0.9895 -Training loss: 0.1094 - Val acc: 0.0538 - Val loss: 13.0828 - Time: 115.7868s 
Epoch 6 - Training acc: 0.9907 -Training loss: 0.0911 - Val acc: 0.0538 - Val loss: 14.8426 - Time: 116.9571s 
Epoch 7 - Training acc: 0.9875 -Training loss: 0.1007 - Val acc: 0.0538 - Val loss: 17.3698 - Time: 115.7701s 
Epoch 8 - Training acc: 0.9910 -Training loss: 0.0954 - Val acc: 0.0538 - Val loss: 11.3305 - Time: 116.7785s 
Epoch 9 - Training acc: 0.9873 -Training loss: 0.1053 - Val acc: 0.0538 - Val loss: 14.8059 - Time: 118.3686s 
Epoch 10 - Training acc: 0.9922 -Training loss: 0.0832 - Val acc: 0.0538 - Val loss: 11.6256 - Time: 121.7628s 
Training completed!
