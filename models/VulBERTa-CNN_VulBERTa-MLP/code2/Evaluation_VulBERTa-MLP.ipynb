{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "import torch\n",
    "import sklearn\n",
    "import os\n",
    "import random\n",
    "import custom\n",
    "import models\n",
    "import regex\n",
    "import clang\n",
    "from clang import *\n",
    "from clang import cindex\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM, RobertaForSequenceClassification\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from custom import CustomDataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Set default device (GPU or CPU)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deterministic/reproducible flags\n",
    "\n",
    "seedlist = [42, 834, 692, 489, 901, 408, 819, 808, 531, 166]\n",
    "\n",
    "seed = seedlist[5]\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights and Biases flags\n",
    "\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['WANDB_MODE'] = 'dryrun'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "#os.environ['WANDB_NOTEBOOK_NAME'] = 'Pretrain word-level VulBERTa on Draper'\n",
    "#os.environ['WANDB_NAME'] = 'linux'\n",
    "#os.environ['WANDB_PROJECT'] = 'projectName'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/initialise custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer\n",
    "\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import NormalizedString,PreTokenizedString\n",
    "from typing import List \n",
    "\n",
    "class MyTokenizer:\n",
    "    \n",
    "    cidx = cindex.Index.create()\n",
    "        \n",
    "\n",
    "    def clang_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "        ## Tokkenize using clang\n",
    "        tok = []\n",
    "        tu = self.cidx.parse('tmp.c',\n",
    "                       args=[''],  \n",
    "                       unsaved_files=[('tmp.c', str(normalized_string.original))],  \n",
    "                       options=0)\n",
    "        for t in tu.get_tokens(extent=tu.cursor.extent):\n",
    "            spelling = t.spelling.strip()\n",
    "            \n",
    "            if spelling == '':\n",
    "                continue\n",
    "                \n",
    "            ## Keyword no need\n",
    "\n",
    "            ## Punctuations no need\n",
    "\n",
    "            ## Literal all to BPE\n",
    "            \n",
    "            #spelling = spelling.replace(' ', '')\n",
    "            tok.append(NormalizedString(spelling))\n",
    "\n",
    "        return(tok)\n",
    "    \n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        pretok.split(self.clang_split)\n",
    "        \n",
    "## Custom tokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import normalizers,decoders\n",
    "from tokenizers.normalizers import StripAccents, unicode_normalizer_from_str, Replace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import processors,pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "## Init new tokenizers\n",
    "#my_tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "#my_tokenizer = Tokenizer(BPE())\n",
    "\n",
    "\n",
    "## Load pre-trained tokenizers\n",
    "vocab, merges = BPE.read_file(vocab=\"./tokenizer/drapgh-vocab.json\", merges=\"./tokenizer/drapgh-merges.txt\")\n",
    "my_tokenizer = Tokenizer(BPE(vocab, merges, unk_token=\"<unk>\"))\n",
    "\n",
    "my_tokenizer.normalizer = normalizers.Sequence([StripAccents(), Replace(\" \", \"Ã„\")])\n",
    "my_tokenizer.pre_tokenizer = PreTokenizer.custom(MyTokenizer())\n",
    "my_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "my_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    special_tokens=[\n",
    "    (\"<s>\",0),\n",
    "    (\"<pad>\",1),\n",
    "    (\"</s>\",2),\n",
    "    (\"<unk>\",3),\n",
    "    (\"<mask>\",4)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose and prepare testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose the dataset ('draper','vuldeepecker','devign','reveal')\n",
    "mydataset = 'devign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer.enable_truncation(max_length=1024)\n",
    "my_tokenizer.enable_padding(direction='right', pad_id=1, pad_type_id=0, pad_token='<pad>', length=None, pad_to_multiple_of=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encodings(encodings):\n",
    "    input_ids=[]\n",
    "    attention_mask=[]\n",
    "    for enc in encodings:\n",
    "        input_ids.append(enc.ids)\n",
    "        attention_mask.append(enc.attention_mask)\n",
    "    return {'input_ids':input_ids, 'attention_mask':attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(code):\n",
    "    ## Remove code comments\n",
    "    pat = re.compile(r'(/\\*([^*]|(\\*+[^*/]))*\\*+/)|(//.*)')\n",
    "    code = re.sub(pat,'',code)\n",
    "    code = re.sub('\\n','',code)\n",
    "    code = re.sub('\\t','',code)\n",
    "    return(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, index):\n",
    "        self.encodings = encodings\n",
    "        #print(encodings)\n",
    "        self.labels = labels\n",
    "        self.index=index\n",
    "        #print(index)\n",
    "        assert len(self.encodings['input_ids']) == len(self.encodings['attention_mask']) ==  len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['index']=idx\n",
    "        #print('retrieving:')\n",
    "        #print(idx)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8196, 8, 10, 24587, 16396, 15, 24597, 22, 16408, 16409, 8218, 16411, 24600, 24602, 16414, 8226, 24611, 24612, 24615, 16425, 8233, 16427, 24620, 16435, 51, 24632, 8249, 8250, 24635, 62, 8255, 24644, 8262, 71, 24649, 24650, 24651, 75, 24654, 16464, 8274, 24659, 8276, 16468, 88, 8284, 24673, 16484, 16487, 8297, 24682, 24688, 16500, 24695, 16503, 8311, 24699, 16508, 125, 16510, 127, 16511, 16515, 24711, 137, 24716, 143, 16527, 8337, 146, 16533, 24726, 24727, 24728, 155, 16541, 161, 162, 166, 24743, 8366, 174, 16558, 24757, 8374, 183, 24758, 16569, 16570, 24763, 24764, 188, 16577, 16578, 16582, 16584, 16596, 16597, 24790, 8419, 227, 24806, 8424, 235, 8428, 24813, 24815, 16624, 8440, 16633, 24827, 253, 24830, 255, 257, 8449, 8451, 8456, 16648, 24842, 8460, 268, 274, 16659, 16660, 8469, 24859, 24863, 24865, 292, 24872, 298, 300, 16685, 301, 24885, 311, 16698, 319, 16704, 16709, 24903, 8527, 16721, 24918, 342, 345, 346, 8538, 16731, 16737, 354, 355, 24935, 16744, 24937, 24938, 24939, 368, 16753, 8562, 16758, 16759, 8566, 24953, 24950, 378, 16765, 8574, 8573, 8581, 390, 24966, 16776, 16777, 24965, 395, 396, 24976, 24978, 24985, 8602, 16804, 420, 427, 25007, 8624, 16816, 8625, 25011, 8626, 16821, 16817, 443, 446, 449, 452, 8647, 16850, 16856, 25049, 8666, 25051, 25058, 16868, 25071, 497, 498, 25077, 8694, 16891, 8702, 16894, 16895, 16897, 514, 16898, 25090, 25089, 512, 8712, 521, 25098, 16909, 526, 8719, 16916, 16917, 8726, 534, 8727, 16921, 8724, 536, 8735, 25123, 16932, 555, 8750, 16943, 16942, 16947, 16948, 8758, 25146, 578, 25157, 16966, 16967, 583, 16969, 16977, 16981, 16988, 25184, 8801, 8802, 8803, 8809, 25194, 25197, 17006, 25206, 8827, 17020, 17024, 25219, 17028, 8837, 8838, 646, 17033, 17036, 653, 25232, 25234, 8853, 25238, 668, 670, 8862, 8864, 17056, 25256, 8873, 682, 8874, 8881, 25266, 691, 25271, 696, 25278, 25280, 17088, 705, 17091, 712, 713, 8906, 8915, 25306, 734, 8930, 747, 25324, 25325, 754, 17140, 757, 17144, 768, 25347, 25348, 771, 17157, 772, 8971, 782, 8978, 25363, 8980, 25368, 795, 25375, 17183, 25383, 17191, 9001, 17193, 17195, 25388, 814, 815, 9024, 838, 25414, 9032, 17224, 17227, 850, 854, 859, 9051, 17245, 863, 17248, 17249, 17251, 867, 17259, 882, 25460, 9079, 9082, 9083, 25468, 25466, 9089, 25477, 25481, 17289, 25489, 914, 17299, 9110, 25494, 17305, 25498, 25500, 25506, 9130, 17323, 25519, 9142, 17336, 25528, 9146, 17338, 17355, 17358, 25551, 979, 25555, 17367, 17375, 17376, 25570, 17378, 25576, 25577, 17386, 1003, 9192, 17388, 9198, 25585, 25586, 17398, 9207, 17402, 9211, 1018, 1021, 17407, 25600, 9218, 9220, 1033, 9226, 9227, 25610, 9229, 1042, 25622, 25624, 25625, 9241, 9248, 9249, 9250, 17447, 1064, 1065, 17448, 25644, 17454, 17457, 17458, 1075, 1076, 17463, 1088, 1090, 1091, 17478, 25676, 1101, 25682, 17492, 25686, 25687, 9307, 25694, 17505, 25706, 1131, 1132, 25710, 9327, 25713, 1138, 25714, 9333, 9334, 17525, 25717, 25724, 1150, 1151, 25726, 9346, 17539, 25732, 17544, 1161, 9356, 9358, 9359, 25744, 25743, 17558, 17562, 1180, 9375, 17571, 1193, 25769, 9390, 25775, 1199, 17585, 25777, 17588, 25782, 9400, 25785, 17594, 1211, 17600, 9408, 1219, 17604, 1226, 9427, 17621, 17622, 17623, 9439, 9441, 1249, 17636, 17637, 17639, 17645, 25839, 17647, 1265, 9458, 9460, 17654, 9464, 1274, 1275, 9466, 1278, 17663, 25856, 17665, 9481, 9483, 1292, 17677, 9495, 17688, 17692, 17693, 25889, 17698, 9512, 9514, 9515, 1324, 17713, 9522, 9523, 1337, 25913, 17721, 25921, 9538, 1347, 9540, 17734, 17739, 1356, 25934, 1361, 17749, 1366, 17751, 25944, 1372, 25951, 17767, 17768, 1385, 25962, 1387, 1388, 9577, 9579, 9583, 9585, 9586, 17779, 17783, 25975, 25980, 25982, 1407, 1409, 25986, 17795, 1410, 1416, 1418, 9611, 9613, 1422, 26007, 17818, 9627, 1441, 26018, 17825, 1447, 9642, 1451, 26027, 17835, 9649, 17842, 1459, 26037, 9659, 9660, 9661, 26049, 17858, 9666, 9669, 17866, 9678, 17871, 17878, 9688, 9689, 17889, 9697, 9699, 17892, 1509, 17894, 9701, 9700, 9712, 1528, 26114, 17925, 26118, 26122, 17933, 9743, 9745, 17938, 9747, 9748, 9749, 17943, 1563, 9756, 1565, 26141, 9759, 1571, 1572, 26149, 17958, 9765, 1578, 17964, 26156, 9774, 9775, 9779, 9780, 26174, 17984, 9793, 17986, 9795, 26180, 17989, 1608, 1609, 17993, 17998, 18003, 18005, 1622, 26199, 26202, 26203, 1642, 9835, 18028, 26223, 18032, 18034, 26227, 26230, 26237, 18045, 9860, 9861, 1673, 9866, 1675, 1677, 18061, 9872, 18064, 1683, 9880, 18072, 18074, 1688, 18078, 9889, 18081, 9890, 26275, 9893, 9894, 1701, 1706, 9900, 9901, 9904, 18102, 9911, 26296, 26300, 18111, 9919, 18114, 18127, 18129, 18130, 26321, 18133, 1750, 26331, 18146, 18149, 26346, 18154, 9964, 1776, 9969, 1787, 18172, 26368, 18179, 26372, 1798, 18183, 1800, 26379, 26381, 18191, 10004, 26389, 26394, 10013, 10015, 1826, 10018, 10020, 1830, 18217, 1836, 18221, 26414, 10034, 26423, 18232, 26426, 1851, 18234, 18238, 26430, 1856, 10056, 10060, 26445, 1871, 1872, 18257, 26452, 1882, 10075, 26459, 10077, 26463, 26464, 26465, 18278, 18283, 26479, 18290, 18298, 1914, 18300, 1915, 10110, 10115, 10124, 26509, 18320, 26516, 10133, 10136, 26521, 18328, 10137, 26522, 26525, 26527, 10146, 1956, 10149, 18342, 1960, 26537, 1963, 18347, 18349, 10159, 18353, 18356, 1973, 18361, 18366, 26566, 26569, 10187, 26573, 26576, 10193, 18390, 26584, 2014, 2017, 18402, 10212, 18405, 10215, 26600, 18411, 10221, 10222, 18413, 26606, 2034, 18419, 26614, 26616, 18425, 10232, 18427, 26619, 10239, 10240, 26629, 2055, 18451, 2068, 26646, 26647, 10264, 18464, 10274, 2082, 18472, 26665, 26664, 2091, 10283, 10288, 18481, 26676, 10293, 18488, 18497, 18501, 18503, 2120, 10317, 26715, 18524, 2143, 18528, 2147, 26727, 26728, 18541, 2158, 10349, 2160, 18543, 26739, 10356, 26746, 18557, 18560, 10369, 18561, 26754, 2183, 10376, 26762, 18571, 10383, 18580, 10389, 10391, 10399, 2209, 26791, 10409, 18603, 10412, 18613, 10422, 26807, 26808, 18615, 2235, 18621, 2238, 26818, 18627, 10439, 2253, 18643, 2260, 10462, 18660, 10470, 10471, 26856, 2285, 26870, 26875, 10494, 10501, 26892, 26893, 18702, 18706, 2326, 2330, 10522, 26906, 2335, 2336, 18723, 2350, 2352, 18736, 26933, 26934, 18749, 2367, 18753, 18756, 2376, 26953, 18766, 10575, 2385, 10579, 26965, 18776, 26969, 10590, 2399, 10592, 10596, 10599, 26983, 26984, 18795, 2411, 10606, 2414, 10608, 10611, 26997, 18806, 10622, 18814, 2431, 27008, 18828, 27025, 27026, 18834, 2450, 27036, 10659, 18852, 10662, 10673, 2483, 27060, 10679, 18872, 10683, 27067, 27069, 2493, 10685, 2496, 27072, 18883, 10692, 2503, 2507, 18891, 2508, 18898, 27095, 2521, 18905, 27102, 27104, 2529, 10722, 18916, 10728, 10729, 27115, 10736, 18930, 10739, 2548, 27131, 27135, 2560, 2559, 2561, 18943, 10751, 27137, 27142, 18946, 10762, 18955, 18959, 18965, 2582, 2584, 10778, 27164, 10783, 2596, 18990, 27183, 2609, 18994, 27187, 10803, 18997, 18999, 19002, 27194, 10813, 19006, 27199, 10815, 19009, 10820, 19012, 19014, 10828, 27216, 10835, 19031, 10840, 2651, 10843, 27242, 19051, 10860, 2669, 10862, 2671, 27247, 10867, 2676, 19062, 19065, 2681, 27259, 2685, 27264, 27269, 10889, 27275, 27276, 19087, 10897, 2713, 27290, 19099, 10912, 19120, 10928, 19122, 10932, 19130, 10938, 10941, 2753, 19137, 19142, 2763, 10956, 10960, 10964, 19156, 10965, 10970, 19165, 2789, 19177, 10986, 2795, 2797, 19181, 2800, 2805, 2808, 2811, 11006, 2815, 19201, 11019, 19213, 11023, 11027, 19226, 19227, 2844, 19233, 2862, 11055, 19248, 19250, 19252, 2871, 11066, 11069, 19264, 11075, 2892, 19278, 11087, 19281, 19282, 11095, 19294, 2920, 11124, 2936, 11132, 19325, 2943, 2948, 11151, 11155, 2964, 2965, 11158, 11160, 2973, 19357, 19361, 2978, 2981, 19366, 2982, 2985, 19371, 19372, 11194, 11197, 3013, 3019, 19406, 19414, 19423, 3039, 3047, 11241, 19435, 11248, 3059, 3061, 11254, 11261, 3071, 11263, 11264, 11270, 19462, 19472, 19476, 19479, 11288, 19480, 11293, 11295, 3105, 3106, 19489, 11309, 19504, 11313, 11314, 3124, 11326, 19520, 19523, 19525, 19530, 11343, 19539, 11349, 11354, 19547, 19549, 11357, 11373, 19571, 3187, 3189, 19573, 3192, 11384, 3195, 11388, 3207, 11402, 3215, 19603, 3219, 11414, 3224, 19615, 3232, 3234, 19620, 19625, 3249, 11446, 3255, 11448, 11450, 11451, 19648, 11457, 3270, 19659, 3275, 11469, 19662, 19666, 3284, 19674, 19682, 11492, 3302, 11497, 19689, 11498, 3309, 11504, 3312, 19707, 11517, 19712, 3328, 11528, 3339, 19725, 3344, 19734, 3356, 11549, 19748, 3365, 19751, 3368, 19752, 19758, 11570, 19766, 3383, 19785, 11595, 19788, 11597, 19791, 11611, 11612, 11613, 11616, 11628, 3437, 3436, 19823, 19825, 19833, 19835, 3461, 19845, 3464, 11657, 3471, 3472, 3477, 19864, 19866, 19875, 11683, 3494, 11687, 11688, 11691, 11692, 11693, 11696, 19893, 19907, 19910, 3526, 3530, 11723, 19919, 11731, 11732, 11735, 3546, 19932, 19933, 19935, 11745, 3560, 19949, 3566, 3568, 3569, 19963, 3585, 19970, 3600, 19991, 19993, 19996, 3616, 3619, 3620, 20009, 11825, 20017, 3637, 3638, 20023, 11841, 20034, 20036, 11847, 20040, 3662, 20055, 20056, 11863, 20060, 3678, 11871, 20064, 20067, 20068, 11882, 11883, 3690, 20077, 3698, 11895, 11899, 20098, 11912, 20106, 11914, 11927, 3738, 11930, 20130, 3747, 11941, 20136, 11945, 20138, 11947, 3757, 20144, 3761, 11955, 11963, 3771, 11965, 3774, 11968, 3786, 11981, 20174, 3789, 11984, 20179, 20181, 3799, 3800, 20184, 20183, 12004, 12006, 3814, 3819, 20204, 3820, 20214, 12030, 20222, 12041, 20240, 20241, 12049, 3859, 20243, 12053, 3858, 12055, 12056, 3873, 12068, 12082, 12093, 12094, 20288, 12098, 3907, 12101, 3909, 12105, 12109, 3917, 3921, 12117, 12121, 12123, 20321, 12132, 12137, 12139, 3955, 12148, 3968, 3969, 3974, 12166, 20366, 20367, 3992, 3994, 20385, 12194, 20395, 12203, 20402, 12211, 12212, 20403, 20407, 4026, 4030, 20417, 4034, 12225, 12231, 20436, 20444, 4061, 12259, 4071, 4072, 20455, 4076, 4081, 20472, 4090, 20478, 4102, 12298, 4111, 4112, 20497, 12308, 12310, 4118, 4121, 12317, 4125, 12322, 12325, 4137, 4144, 20530, 20532, 20536, 12351, 20550, 4170, 12364, 20557, 12373, 12375, 12379, 12386, 4194, 4202, 12396, 12397, 12399, 20596, 20603, 4219, 12418, 12419, 20610, 20614, 4232, 12426, 4239, 4242, 12440, 20638, 12446, 4258, 20642, 4263, 4268, 4270, 20657, 4273, 20670, 20676, 12485, 4294, 20692, 4318, 4321, 12518, 12520, 20712, 12522, 20720, 4336, 20725, 20726, 4343, 4346, 12542, 4351, 4352, 4354, 20744, 4373, 12567, 4380, 4381, 12579, 20772, 20773, 20777, 20779, 4408, 4414, 4421, 12614, 4424, 4427, 12628, 20823, 20826, 4443, 4442, 4447, 12648, 12651, 4461, 12664, 12666, 4478, 12671, 12674, 20869, 12680, 12685, 4494, 4495, 20879, 20883, 12702, 4511, 20895, 4514, 20899, 20898, 4522, 20906, 4525, 12718, 20910, 4530, 20916, 12727, 20929, 12745, 4557, 12762, 12765, 20957, 4575, 4578, 20963, 4580, 12775, 4583, 12778, 4594, 20981, 4598, 4599, 4600, 20987, 12796, 4606, 20992, 12801, 4612, 20997, 21006, 21008, 12817, 21009, 12823, 21020, 12829, 12830, 4636, 4640, 21026, 4647, 21033, 4650, 4654, 21045, 12854, 4665, 12862, 4674, 21059, 4676, 12869, 4679, 12871, 21068, 4686, 4701, 21091, 4708, 4713, 4723, 12921, 4729, 4734, 21124, 12936, 12938, 4757, 21141, 21148, 4767, 12963, 12966, 21160, 4783, 21171, 21173, 21176, 4794, 12988, 21189, 12999, 21196, 21200, 4816, 4818, 13016, 13017, 4830, 21215, 4832, 13027, 21222, 13034, 13036, 21230, 13039, 4848, 4847, 21236, 13046, 21238, 4861, 21251, 13060, 4876, 4886, 13083, 4891, 21280, 21282, 13091, 4903, 21288, 21290, 4909, 4913, 4916, 13110, 21313, 13121, 4930, 21319, 4937, 13135, 4945, 4948, 4949, 21334, 13147, 4958, 4960, 21345, 4971, 4973, 13167, 4976, 13176, 21369, 21372, 13180, 13182, 4991, 4992, 13186, 4996, 5002, 21389, 21390, 5006, 5008, 21399, 5020, 13213, 21407, 5027, 13223, 13225, 21417, 5036, 5041, 5048, 21434, 5051, 5052, 21436, 5055, 13248, 21440, 13250, 5066, 21453, 5072, 13270, 5081, 21468, 5085, 21470, 5087, 13278, 13282, 5091, 5098, 21483, 13291, 13293, 5101, 13302, 13303, 13307, 5116, 5122, 13324, 5137, 13334, 13337, 21531, 13346, 5156, 5157, 5158, 13360, 13361, 21555, 13367, 5176, 21561, 5177, 5178, 13372, 5180, 13373, 13375, 21564, 13377, 13385, 5195, 5197, 13389, 21586, 21587, 13396, 21589, 21601, 21607, 5230, 13424, 21619, 13427, 21621, 5239, 21627, 5248, 21633, 5252, 5255, 13449, 5264, 13457, 13459, 13460, 13467, 5275, 21664, 5289, 5290, 13483, 5292, 21683, 21685, 13499, 21691, 21704, 13515, 13521, 5333, 13529, 21726, 21738, 13546, 5359, 13552, 21746, 5363, 5366, 13565, 21760, 13569, 5379, 5380, 21765, 13592, 5402, 5403, 13595, 5407, 5409, 13605, 13612, 5421, 13623, 21815, 5439, 21823, 5446, 13648, 13649, 5459, 21843, 5460, 5464, 5467, 13660, 5470, 21858, 21860, 21864, 13673, 5484, 13678, 21871, 5487, 13683, 21876, 21875, 21880, 21883, 21885, 5503, 5510, 5517, 21901, 13713, 5521, 5523, 13714, 5526, 5529, 21914, 13725, 21918, 21921, 13738, 13743, 13753, 21948, 5566, 21951, 13758, 5570, 5573, 13773, 21972, 21974, 5596, 21980, 5601, 13802, 5611, 22002, 13812, 13820, 5634, 5638, 5640, 13833, 13836, 22028, 22029, 22031, 5648, 22043, 5660, 22045, 13857, 22049, 5666, 5674, 22060, 13869, 5682, 13886, 22080, 22081, 22083, 13893, 13896, 22091, 5713, 22097, 5715, 22104, 22105, 13914, 22109, 5729, 22119, 22124, 5741, 13938, 5757, 22142, 22143, 5761, 5768, 5790, 22184, 22187, 22189, 22190, 22192, 14001, 5815, 5816, 5825, 5827, 5829, 22213, 5831, 14027, 14030, 22223, 5840, 22222, 14034, 22226, 5847, 5852, 22236, 14044, 14046, 5860, 22244, 5867, 5869, 14063, 5874, 22261, 5880, 22269, 5887, 22273, 14088, 14099, 22291, 14102, 5910, 5913, 5915, 5923, 14118, 5928, 14121, 22313, 22315, 5934, 5937, 14130, 14132, 14133, 14136, 22331, 5948, 22333, 22334, 14146, 14147, 22350, 5967, 5970, 14172, 5983, 14176, 22368, 14180, 5995, 5996, 5997, 5999, 14192, 14194, 6005, 6012, 22405, 14214, 6027, 22415, 6034, 22423, 6040, 14231, 14236, 6047, 14240, 14244, 6053, 22437, 22442, 6059, 22445, 14254, 22453, 6072, 22456, 6074, 14267, 22458, 14271, 14272, 6083, 6087, 22477, 22479, 6097, 22482, 22483, 6102, 6107, 22494, 14304, 14312, 22507, 22511, 6129, 6133, 14333, 6153, 6155, 6158, 14354, 22546, 22547, 14360, 22553, 6174, 14367, 22565, 14373, 6183, 6185, 22573, 14386, 6197, 14390, 6199, 6198, 6202, 14396, 22591, 6207, 14402, 14405, 6215, 22600, 14410, 22619, 14427, 14429, 22626, 14445, 22640, 6269, 14464, 6274, 6275, 22664, 14472, 6282, 22666, 6286, 6287, 22688, 14496, 6308, 22697, 6314, 22699, 14509, 6322, 22710, 6339, 22724, 14532, 14535, 14543, 6352, 22739, 6361, 14556, 22753, 14563, 6377, 14570, 22774, 22775, 14587, 6401, 22787, 6404, 6409, 22794, 14611, 6425, 22811, 14622, 6431, 14624, 22818, 14630, 22822, 6440, 22825, 14634, 22828, 6445, 14643, 22837, 14652, 14653, 6461, 14657, 22850, 6467, 14661, 22855, 14671, 14675, 22867, 6491, 6495, 14688, 6504, 22890, 6508, 22895, 14706, 6515, 6517, 22902, 14713, 6523, 6541, 14740, 22937, 6553, 6557, 6563, 14758, 6569, 22954, 22957, 22958, 14770, 14772, 14776, 14781, 22978, 6595, 14788, 14789, 14791, 6609, 14801, 6612, 14808, 6618, 6619, 14818, 14819, 6627, 23011, 6630, 23015, 6632, 23017, 6626, 23019, 14829, 23022, 23027, 6644, 23033, 23034, 6654, 14850, 6661, 14854, 6673, 14870, 6679, 6682, 23070, 23071, 6686, 23075, 14887, 6697, 14890, 6706, 23093, 14902, 6710, 23102, 23109, 14918, 23112, 6740, 14933, 14939, 6748, 14943, 23135, 14947, 23146, 14958, 14962, 14963, 23156, 14965, 23160, 14968, 14971, 23167, 14976, 14980, 23173, 14985, 14988, 6802, 23187, 15000, 6809, 6810, 6811, 6812, 6814, 6815, 6818, 23204, 6826, 15022, 15023, 6833, 15026, 15027, 6835, 6839, 23227, 15041, 6856, 6864, 23249, 6866, 15059, 23251, 23260, 15069, 23263, 6886, 15086, 23279, 23286, 23292, 23298, 6917, 6921, 23308, 15116, 23314, 6931, 23316, 6933, 15130, 6941, 15134, 6960, 6967, 15159, 15163, 15172, 23365, 23366, 6981, 15176, 23368, 15179, 23373, 15183, 15187, 23379, 15191, 23384, 15198, 23394, 23396, 15204, 23404, 7021, 23409, 15217, 15219, 7026, 15222, 23417, 15225, 15227, 15226, 7037, 23426, 23430, 23431, 23432, 23434, 15245, 7055, 23440, 7056, 7059, 23444, 15258, 7070, 15262, 15264, 7073, 7076, 7078, 7083, 15279, 23471, 7089, 7090, 23483, 23488, 15296, 7115, 15311, 15312, 23509, 15319, 23511, 23520, 7138, 23523, 7139, 15330, 15337, 7152, 7157, 23545, 15356, 7164, 15358, 7169, 7171, 7180, 7187, 23575, 23580, 15397, 7206, 23590, 23589, 23594, 15403, 7211, 7218, 15421, 23613, 7231, 23616, 23621, 7250, 15445, 7256, 7263, 15458, 23653, 15463, 23663, 15472, 23665, 15476, 7288, 7295, 15489, 23682, 23686, 23689, 15499, 7308, 15501, 7309, 7316, 15514, 7322, 7325, 23710, 15523, 7331, 15526, 23727, 23731, 23733, 7349, 7352, 7353, 7355, 15550, 7359, 7371, 23756, 15567, 7377, 23761, 7378, 7383, 15577, 15578, 23772, 23774, 15583, 7394, 7400, 7406, 23797, 15612, 23806, 15622, 7434, 15628, 23822, 15635, 7446, 7450, 23840, 23841, 23847, 23849, 15660, 15661, 7469, 7473, 23861, 7484, 23871, 7487, 15686, 7496, 15693, 7502, 23894, 7511, 23903, 7522, 23911, 15719, 7528, 7530, 7531, 7538, 15731, 7540, 7543, 23929, 15748, 15750, 15754, 23949, 15768, 7580, 15778, 7610, 15815, 15820, 24013, 15835, 15844, 24037, 24042, 15852, 24046, 15855, 24048, 15857, 7665, 24053, 15869, 7678, 24063, 24066, 24069, 24070, 15877, 7689, 7690, 24089, 7706, 7709, 15902, 24095, 7712, 7715, 7719, 15918, 24114, 15922, 24117, 7735, 15928, 15931, 7745, 15952, 15954, 7770, 24155, 7772, 24156, 15966, 24160, 24161, 15973, 15976, 24168, 7786, 7791, 7792, 15986, 7797, 15993, 16003, 24199, 7817, 7819, 7826, 24218, 24219, 24221, 7837, 16032, 24224, 24226, 24227, 16036, 24235, 7858, 24251, 16063, 7877, 16074, 16076, 7894, 16086, 24285, 7903, 7905, 24290, 7907, 7908, 7906, 16109, 24308, 24309, 7926, 7929, 16122, 16124, 16126, 24318, 24320, 16132, 24326, 16138, 16140, 7949, 24334, 24336, 7955, 24340, 24339, 7960, 7963, 7969, 16168, 7989, 16184, 16185, 7995, 24379, 7998, 24384, 8006, 24392, 24398, 16214, 8024, 24410, 8030, 24415, 16235, 8044, 24430, 16243, 16247, 24442, 16252, 16254, 24448, 8065, 16257, 16262, 8073, 8078, 16270, 8081, 24469, 16277, 24471, 16279, 8086, 8088, 16283, 8093, 8095, 16290, 16294, 16305, 16307, 24502, 24506, 16319, 24511, 8129, 24514, 24524, 16339, 8149, 24533, 16348, 24543, 24544, 24545, 8162, 24547, 24548, 8160, 16358, 8164, 8168, 16361, 8173, 24558, 8177, 8182, 24570, 8190, 24575}\n",
      "      project                                 commit_id  target  \\\n",
      "8196   FFmpeg  f3ad901a32c95239f302f173b866b82fb1f6cdf9       0   \n",
      "8        qemu  aa1530dec499f7525d2ccaa0e3a876dc8089ed1e       1   \n",
      "10       qemu  21ce148c7ec71ee32834061355a5ecfd1a11f90f       1   \n",
      "24587  FFmpeg  d1adad3cca407f493c3637e20ecd4f7124e69212       0   \n",
      "16396    qemu  7102fa7073b2cefb33ab4012a11f15fbf297a74b       0   \n",
      "...       ...                                       ...     ...   \n",
      "8177     qemu  297a3646c2947ee64a6d42ca264039732c6218e0       1   \n",
      "8182   FFmpeg  c89658008705d949c319df3fa6f400c481ad73e1       0   \n",
      "24570  FFmpeg  f6774f905fb3cfdc319523ac640be30b14c1bc55       1   \n",
      "8190   FFmpeg  ed7fa39c2dd63607fd5c5ed3c607a11a8a33bbe3       0   \n",
      "24575  FFmpeg  435a6082f9e368196e0d8347858c63de1126af2c       0   \n",
      "\n",
      "                                                    func  \n",
      "8196   static inline int compress_coeffs(int *coef, i...  \n",
      "8      static void filter_mirror_setup(NetFilterState...  \n",
      "10     static inline int64_t sub64(const int64_t a, c...  \n",
      "24587  static inline void RENAME(rgb32tobgr24)(const ...  \n",
      "16396  static void pc_compat_0_13(MachineState *machi...  \n",
      "...                                                  ...  \n",
      "8177   void visit_type_uint16(Visitor *v, uint16_t *o...  \n",
      "8182   static void rtsp_parse_transport(RTSPMessageHe...  \n",
      "24570  static int estimate_best_b_count(MpegEncContex...  \n",
      "8190   static int mov_write_avcc_tag(ByteIOContext *p...  \n",
      "24575  static int str_read_packet(AVFormatContext *s,...  \n",
      "\n",
      "[2732 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "if mydataset=='devign':\n",
    "    test_index=set()\n",
    "\n",
    "    with open('data/finetune/devign/valid.txt') as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            #print(line)\n",
    "            #print(int(line))\n",
    "            test_index.add(int(line))\n",
    "    mydata = pd.read_json('data/finetune/devign/Devign.json')\n",
    "    m3=mydata.iloc[list(test_index)]\n",
    "    print(test_index)\n",
    "    print(m3)\n",
    "    mydata = None\n",
    "    del(mydata)\n",
    "    m3.func = m3.func.apply(cleaner)\n",
    "\n",
    "    test_encodings = my_tokenizer.encode_batch(m3.func)\n",
    "    test_encodings = process_encodings(test_encodings)\n",
    "    test_dataset = MyCustomDataset(test_encodings, m3.target.tolist(),m3.index)\n",
    "else:\n",
    "    m3 = pd.read_pickle('data/finetune/%s/%s_test.pkl'%(mydataset,mydataset))\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        m3.functionSource = m3.functionSource.apply(cleaner)\n",
    "        test_encodings = my_tokenizer.encode_batch(m3.functionSource)\n",
    "        test_encodings = process_encodings(test_encodings)\n",
    "        \n",
    "        if  mydataset =='draper':\n",
    "            test_dataset = MyCustomDataset(test_encodings, (m3['combine']*1).tolist())\n",
    "        else:\n",
    "            test_dataset = MyCustomDataset(test_encodings, m3.label.tolist())\n",
    "    except:\n",
    "        m3.func = m3.func.apply(cleaner)\n",
    "        test_encodings = my_tokenizer.encode_batch(m3.func)\n",
    "        test_encodings = process_encodings(test_encodings)\n",
    "        test_dataset = MyCustomDataset(test_encodings, m3.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### D2A ONLY\n",
    "#task = 'function'\n",
    "#m3 = pd.read_csv('data/finetune/%s/%s/d2a_lbv1_%s_val.csv'%(mydataset,task,task))\n",
    "#m3.code = m3.code.apply(cleaner)\n",
    "#test_encodings = my_tokenizer.encode_batch(m3.code)\n",
    "#test_encodings = process_encodings(test_encodings)\n",
    "#test_dataset = MyCustomDataset(test_encodings, m3.label.tolist())\n",
    "\n",
    "\n",
    "###########################test_dataset = MyCustomDataset(test_encodings, [0]*len(m3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load fine-tuned VulBERTa-MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel=mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124836866\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('/home/<ANONYMOUS>/VulBERTa/models/fine_tuned/VB-MLP_devign/')\n",
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_accuracy(probs,all_labels):\n",
    "    #print(probs)\n",
    "    def getClass(x):\n",
    "        return(x.index(max(x)))\n",
    "    \n",
    "    all_labels = all_labels.tolist()\n",
    "    probs_list=probs.tolist()\n",
    "    probs = pd.Series(probs.tolist())\n",
    "    #print(probs)\n",
    "    all_predicted = probs.apply(getClass)\n",
    "    all_predicted.reset_index(drop=True, inplace=True)\n",
    "    vc = pd.value_counts(all_predicted == all_labels)\n",
    "    try:\n",
    "        acc = vc[1]/len(all_labels)\n",
    "    except:\n",
    "        if(vc.index[0]==False):\n",
    "            acc = 0\n",
    "        else:\n",
    "            acc = 1\n",
    "    \n",
    "    #print(acc)\n",
    "    #print(all_predicted)\n",
    "    #print(probs)\n",
    "    return(acc,all_predicted, probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "multigpu=False\n",
    "if multigpu:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_pred=[]\n",
    "all_labels=[]\n",
    "all_probs=[]\n",
    "all_predictions=[]\n",
    "all_index=[]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        all_index+=batch['index']\n",
    "        #print(batch)\n",
    "        #print(all_index)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        acc_val,pred, predictions = softmax_accuracy(torch.nn.functional.softmax(outputs[1],dim=1),labels)\n",
    "        all_pred += pred.tolist()\n",
    "        all_labels += labels.tolist()\n",
    "        all_probs += outputs[1].tolist()\n",
    "        all_predictions += predictions.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[1204  341]\n",
      " [ 588  599]]\n",
      "\n",
      "TP: 599\n",
      "FP: 341\n",
      "TN: 1204\n",
      "FN: 588\n",
      "\n",
      "Accuracy: 0.6599560761346999\n",
      "Precision: 0.6372340425531915\n",
      "Recall: 0.5046335299073293\n",
      "F-measure: 0.5632346027268453\n",
      "Precision-Recall AUC: 0.6949226108207741\n",
      "AUC: 0.7271891009125286\n",
      "MCC: 0.29624773462883225\n"
     ]
    }
   ],
   "source": [
    "confusion = sklearn.metrics.confusion_matrix(y_true=all_labels, y_pred=all_pred)\n",
    "print('Confusion matrix: \\n',confusion)\n",
    "\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "print('\\nTP:',tp)\n",
    "print('FP:',fp)\n",
    "print('TN:',tn)\n",
    "print('FN:',fn)\n",
    "\n",
    "probs2=[]\n",
    "for x in all_probs:\n",
    "    probs2.append(x[1])\n",
    "\n",
    "## Performance measure\n",
    "print('\\nAccuracy: '+ str(sklearn.metrics.accuracy_score(y_true=all_labels, y_pred=all_pred)))\n",
    "print('Precision: '+ str(sklearn.metrics.precision_score(y_true=all_labels, y_pred=all_pred)))\n",
    "print('Recall: '+ str(sklearn.metrics.recall_score(y_true=all_labels, y_pred=all_pred)))\n",
    "print('F-measure: '+ str(sklearn.metrics.f1_score(y_true=all_labels, y_pred=all_pred)))\n",
    "print('Precision-Recall AUC: '+ str(sklearn.metrics.average_precision_score(y_true=all_labels, y_score=probs2)))\n",
    "print('AUC: '+ str(sklearn.metrics.roc_auc_score(y_true=all_labels, y_score=probs2)))\n",
    "print('MCC: '+ str(sklearn.metrics.matthews_corrcoef(y_true=all_labels, y_pred=all_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2732\n",
      "<class 'list'>\n",
      "[0 1 1 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(all_labels))\n",
    "print(type(all_labels))\n",
    "#print(all_labels)\n",
    "print(np.array(all_labels).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "5     0\n",
      "6     0\n",
      "7     1\n",
      "8     0\n",
      "9     1\n",
      "10    0\n",
      "11    1\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    1\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "20    0\n",
      "21    1\n",
      "22    0\n",
      "23    1\n",
      "24    1\n",
      "25    1\n",
      "26    0\n",
      "27    0\n",
      "28    1\n",
      "29    0\n",
      "30    1\n",
      "31    0\n",
      "32    1\n",
      "33    0\n",
      "34    0\n",
      "35    1\n",
      "36    1\n",
      "37    0\n",
      "38    1\n",
      "39    1\n",
      "40    0\n",
      "41    0\n",
      "42    0\n",
      "43    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2732\n",
      "[0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(all_pred))\n",
    "print(all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_accs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e5dd50179b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_accs' is not defined"
     ]
    }
   ],
   "source": [
    "print(all_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)\n",
    "print(acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_predictions))\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
