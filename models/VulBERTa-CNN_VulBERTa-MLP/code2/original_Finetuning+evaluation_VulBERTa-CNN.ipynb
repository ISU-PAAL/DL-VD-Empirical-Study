{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import torchtext.vocab as vocab\n",
    "import sklearn.metrics\n",
    "from transformers import RobertaModel\n",
    "from transformers import RobertaConfig\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.optim import SGD,Adam,RMSprop\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from clang import *\n",
    "\n",
    "\n",
    "seed = 1234\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "MultiGPU:  False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "multigpu = False\n",
    "if device == torch.device('cuda'):\n",
    "\tmultigpu = torch.cuda.device_count() > 1\n",
    "print('Device: ',device)\n",
    "print('MultiGPU: ',multigpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training & vocab parameters\n",
    "DATA_PATH = 'data'\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = VOCAB_SIZE+2\n",
    "EMBED_DIM = 768 #768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer\n",
    "\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import NormalizedString,PreTokenizedString\n",
    "from typing import List \n",
    "\n",
    "class MyTokenizer:\n",
    "    \n",
    "    cidx = cindex.Index.create()\n",
    "        \n",
    "\n",
    "    def clang_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "        ## Tokkenize using clang\n",
    "        tok = []\n",
    "        tu = self.cidx.parse('tmp.c',\n",
    "                       args=[''],  \n",
    "                       unsaved_files=[('tmp.c', str(normalized_string.original))],  \n",
    "                       options=0)\n",
    "        for t in tu.get_tokens(extent=tu.cursor.extent):\n",
    "            spelling = t.spelling.strip()\n",
    "            \n",
    "            if spelling == '':\n",
    "                continue\n",
    "                \n",
    "            ## Keyword no need\n",
    "\n",
    "            ## Punctuations no need\n",
    "\n",
    "            ## Literal all to BPE\n",
    "            \n",
    "            #spelling = spelling.replace(' ', '')\n",
    "            tok.append(NormalizedString(spelling))\n",
    "\n",
    "        return(tok)\n",
    "    \n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        pretok.split(self.clang_split)\n",
    "        \n",
    "## Custom tokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import normalizers,decoders\n",
    "from tokenizers.normalizers import StripAccents, unicode_normalizer_from_str, Replace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import processors,pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "## Load pre-trained tokenizers\n",
    "vocab, merges = BPE.read_file(vocab_filename=\"./tokenizer/drapgh-vocab.json\", merges_filename=\"./tokenizer/drapgh-merges.txt\")\n",
    "my_tokenizer = Tokenizer(BPE(vocab, merges, unk_token=\"<unk>\"))\n",
    "\n",
    "my_tokenizer.normalizer = normalizers.Sequence([StripAccents(), Replace(\" \", \"Ã„\")])\n",
    "my_tokenizer.pre_tokenizer = PreTokenizer.custom(MyTokenizer())\n",
    "my_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "my_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    special_tokens=[\n",
    "    (\"<s>\",0),\n",
    "    (\"<pad>\",1),\n",
    "    (\"</s>\",2),\n",
    "    (\"<unk>\",3),\n",
    "    (\"<mask>\",4)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ONLY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataset = 'devign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer.enable_truncation(max_length=1024)\n",
    "my_tokenizer.enable_padding(direction='right', pad_id=1, pad_type_id=0, pad_token='<pad>', length=None, pad_to_multiple_of=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(code):\n",
    "    ## Remove code comments\n",
    "    pat = re.compile(r'(/\\*([^*]|(\\*+[^*/]))*\\*+/)|(//.*)')\n",
    "    code = re.sub(pat,'',code)\n",
    "    code = re.sub('\\n','',code)\n",
    "    code = re.sub('\\t','',code)\n",
    "    return(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encodings(encodings):\n",
    "    input_ids=[]\n",
    "    attention_mask=[]\n",
    "    for enc in encodings:\n",
    "        input_ids.append(enc.ids)\n",
    "        attention_mask.append(enc.attention_mask)\n",
    "    return {'input_ids':input_ids, 'attention_mask':attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_colname(x):\n",
    "    try:\n",
    "        x = x.rename(columns={'functionSource': \"func\"})\n",
    "    except:\n",
    "        None\n",
    "\n",
    "    try:\n",
    "        x = x.rename(columns={'code': \"func\"})\n",
    "    except:\n",
    "        None\n",
    "\n",
    "    try:\n",
    "        x = x.rename(columns={'label': \"target\"})\n",
    "    except:\n",
    "        None\n",
    "    return(x)\n",
    "\n",
    "if mydataset =='devign':\n",
    "    if TEST_ONLY:\n",
    "        \n",
    "        test_index=set()\n",
    "        with open('data/finetune/devign/test.txt') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                test_index.add(int(line))\n",
    "        mydata = pd.read_json('data/finetune/devign/Devign.json')\n",
    "        m3=mydata.iloc[list(test_index)]\n",
    "        mydata = None\n",
    "        del(mydata)\n",
    "        \n",
    "    else:\n",
    "        train_index=set()\n",
    "        valid_index=set()\n",
    "        test_index=set()\n",
    "\n",
    "        with open('data/finetune/devign/train.txt') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                train_index.add(int(line))\n",
    "\n",
    "        with open('data/finetune/devign/valid.txt') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                valid_index.add(int(line))\n",
    "\n",
    "        with open('data/finetune/devign/test.txt') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                test_index.add(int(line))\n",
    "\n",
    "        mydata = pd.read_json('data/finetune/devign/Devign.json')\n",
    "        m1=mydata.iloc[list(train_index)]\n",
    "        m2=mydata.iloc[list(valid_index)]\n",
    "        m3=mydata.iloc[list(test_index)]\n",
    "\n",
    "        mydata = None\n",
    "        del(mydata)\n",
    "    \n",
    "\n",
    "elif mydataset =='d2a':\n",
    "    task = 'function'\n",
    "    \n",
    "    if TEST_ONLY:\n",
    "        m3 = pd.read_csv('data/finetune/%s/%s/d2a_lbv1_%s_dev.csv'%(mydataset,task,task))\n",
    "        m3 = replace_colname(m3)\n",
    "    else:\n",
    "        m1 = pd.read_csv('data/finetune/%s/%s/d2a_lbv1_%s_train.csv'%(mydataset,task,task))\n",
    "        m2 = pd.read_csv('data/finetune/%s/%s/d2a_lbv1_%s_dev.csv'%(mydataset,task,task))\n",
    "        m3 = pd.read_csv('data/finetune/%s/%s/d2a_lbv1_%s_test.csv'%(mydataset,task,task))\n",
    "       \n",
    "        m1 = replace_colname(m1)\n",
    "        m2 = replace_colname(m2)\n",
    "        m3 = replace_colname(m3)\n",
    "        \n",
    "        \n",
    "else:\n",
    "    \n",
    "    def replace_colname(x):\n",
    "        try:\n",
    "            x = x.rename(columns={'functionSource': \"func\"})\n",
    "        except:\n",
    "            None\n",
    "            \n",
    "        try:\n",
    "            x = x.rename(columns={'code': \"func\"})\n",
    "        except:\n",
    "            None\n",
    "\n",
    "        try:\n",
    "            x = x.rename(columns={'label': \"target\"})\n",
    "        except:\n",
    "            None\n",
    "        return(x)\n",
    "    \n",
    "    \n",
    "    if TEST_ONLY:\n",
    "        m3 = pd.read_pickle('data/finetune/%s/%s_test.pkl'%(mydataset,mydataset))\n",
    "        m3 = replace_colname(m3)\n",
    "        \n",
    "    else:\n",
    "        m1 = pd.read_pickle('data/finetune/%s/%s_train.pkl'%(mydataset,mydataset))\n",
    "        m2 = pd.read_pickle('data/finetune/%s/%s_val.pkl'%(mydataset,mydataset))\n",
    "        m3 = pd.read_pickle('data/finetune/%s/%s_test.pkl'%(mydataset,mydataset))\n",
    "\n",
    "        m1 = replace_colname(m1)\n",
    "        m2 = replace_colname(m2)\n",
    "        m3 = replace_colname(m3)\n",
    "\n",
    "if TEST_ONLY:\n",
    "    m3.func = m3.func.apply(cleaner)\n",
    "    test_encodings = my_tokenizer.encode_batch(m3.func)\n",
    "    try:\n",
    "        test_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(test_encodings,m3.target.tolist())]\n",
    "    except:\n",
    "        test_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(test_encodings,(m3['combine']*1).tolist())]\n",
    "\n",
    "else:\n",
    "    \n",
    "    m1.func = m1.func.apply(cleaner)\n",
    "    train_encodings = my_tokenizer.encode_batch(m1.func)\n",
    "    try:\n",
    "        train_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(train_encodings,m1.target.tolist())]\n",
    "    except:\n",
    "        train_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(train_encodings,(m1['combine']*1).tolist())]\n",
    "\n",
    "\n",
    "    m2.func = m2.func.apply(cleaner)\n",
    "    val_encodings = my_tokenizer.encode_batch(m2.func)\n",
    "    try:\n",
    "        val_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(val_encodings,m2.target.tolist())]\n",
    "    except:\n",
    "        val_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(val_encodings,(m2['combine']*1).tolist())]\n",
    "\n",
    "        \n",
    "    m3.func = m3.func.apply(cleaner)\n",
    "    test_encodings = my_tokenizer.encode_batch(m3.func)\n",
    "    try:\n",
    "        test_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(test_encodings,m3.target.tolist())]\n",
    "    except:\n",
    "        test_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(test_encodings,(m3['combine']*1).tolist())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODES = torchtext.data.Field(batch_first=True, fix_length=1024,use_vocab=False)\n",
    "LABEL = torchtext.data.LabelField(dtype=torch.long, is_target=True,use_vocab=False)\n",
    "fields = {'func': ('codes', CODES), 'target': ('label', LABEL)}\n",
    "\n",
    "class TabularDataset_From_List(torchtext.data.Dataset):\n",
    "    def __init__(self, input_list, format, fields, skip_header=False, **kwargs):\n",
    "        make_example = {\n",
    "            'json': torchtext.data.Example.fromJSON, 'dict': torchtext.data.Example.fromdict}[format.lower()]\n",
    "\n",
    "        examples = [make_example(item, fields) for item in input_list]\n",
    "\n",
    "        if make_example in (torchtext.data.Example.fromdict, torchtext.data.Example.fromJSON):\n",
    "            fields, field_dict = [], fields\n",
    "            for field in field_dict.values():\n",
    "                if isinstance(field, list):\n",
    "                    fields.extend(field)\n",
    "                else:\n",
    "                    fields.append(field)\n",
    "\n",
    "        super(TabularDataset_From_List, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, path=None, root='.data', train=None, validation=None,\n",
    "               test=None, **kwargs):\n",
    "        if path is None:\n",
    "            path = cls.download(root)\n",
    "        train_data = None if train is None else cls(\n",
    "            train, **kwargs)\n",
    "        val_data = None if validation is None else cls(\n",
    "            validation, **kwargs)\n",
    "        test_data = None if test is None else cls(\n",
    "            test, **kwargs)\n",
    "        return tuple(d for d in (train_data, val_data, test_data)\n",
    "                     if d is not None)\n",
    "\n",
    "\n",
    "## Import the 100K data as TabularDataset\n",
    "\n",
    "if TEST_ONLY:\n",
    "    test_data = TabularDataset_From_List(test_encodings,'dict',fields = fields)\n",
    "else:\n",
    "    train_data = TabularDataset_From_List(train_encodings,'dict',fields = fields)\n",
    "    val_data = TabularDataset_From_List(val_encodings,'dict',fields = fields)\n",
    "    test_data = TabularDataset_From_List(test_encodings,'dict',fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = VOCAB_SIZE\n",
    "\n",
    "# place into iterators\n",
    "\n",
    "if TEST_ONLY:\n",
    "    test_iterator = torchtext.data.BucketIterator(\n",
    "        test_data, \n",
    "        batch_size = 1,\n",
    "        sort = False,\n",
    "        shuffle = False)\n",
    "    \n",
    "else:\n",
    "    train_iterator, valid_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "        (train_data, val_data, test_data), \n",
    "        batch_size = BATCH_SIZE,\n",
    "        sort = False,\n",
    "        shuffle = False)\n",
    "\n",
    "UNK_IDX = 3\n",
    "PAD_IDX = 1\n",
    "\n",
    "# test_iterator = torchtext.data.BucketIterator(\n",
    "#     test_data, \n",
    "#     batch_size = BATCH_SIZE,\n",
    "#     sort = False,\n",
    "#     shuffle = False)\n",
    "\n",
    "#from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "# val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "# test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define VulBERTa-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCNN(nn.Module):\n",
    "    def __init__(self, EMBED_SIZE, EMBED_DIM):\n",
    "        super(myCNN,self).__init__()\n",
    "        \n",
    "        pretrained_weights = RobertaModel.from_pretrained('/home/<ANONYMOUS>/VulBERTa/models/pre_trained/').embeddings.word_embeddings.weight\n",
    "\n",
    "        self.embed = nn.Embedding.from_pretrained(pretrained_weights,\n",
    "                                                  freeze=True,\n",
    "                                                  padding_idx=1)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=EMBED_DIM, out_channels=200, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=EMBED_DIM, out_channels=200, kernel_size=4)\n",
    "        self.conv3 = nn.Conv1d(in_channels=EMBED_DIM, out_channels=200, kernel_size=5)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(200*3,256) #500\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = F.relu(self.conv2(x))\n",
    "        x3 = F.relu(self.conv3(x))\n",
    "        \n",
    "        x1 = F.max_pool1d(x1, x1.shape[2])\n",
    "        x2 = F.max_pool1d(x2, x2.shape[2])\n",
    "        x3 = F.max_pool1d(x3, x3.shape[2])\n",
    "        \n",
    "        x = torch.cat([x1,x2,x3],dim=1)\n",
    "        \n",
    "        # flatten the tensor\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        # apply mean over the last dimension\n",
    "        #x = torch.mean(x, -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/<ANONYMOUS>/VulBERTa/models/pre_trained/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = myCNN(EMBED_SIZE,EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.embed.weight.data[UNK_IDX] = torch.zeros(EMBED_DIM)\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myCNN(\n",
      "  (embed): Embedding(50000, 768, padding_idx=1)\n",
      "  (conv1): Conv1d(768, 200, kernel_size=(3,), stride=(1,))\n",
      "  (conv2): Conv1d(768, 200, kernel_size=(4,), stride=(1,))\n",
      "  (conv3): Conv1d(768, 200, kernel_size=(5,), stride=(1,))\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=600, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if multigpu:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of trainable param:  2030810\n"
     ]
    }
   ],
   "source": [
    "print('Num of trainable param: ',sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "\n",
    "try:\n",
    "    cw = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced',classes=[0,1],y=m1.label.tolist())\n",
    "except:\n",
    "    cw = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced',classes=[0,1],y=m1.target.tolist())\n",
    "    \n",
    "c_weights = torch.FloatTensor([cw[0], cw[1]])\n",
    "criterion = nn.CrossEntropyLoss(weight=c_weights)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_accuracy(probs,all_labels):\n",
    "    def getClass(x):\n",
    "        return(x.index(max(x)))\n",
    "    \n",
    "    all_labels = all_labels.tolist()\n",
    "    probs = pd.Series(probs.tolist())\n",
    "    all_predicted = probs.apply(getClass)\n",
    "    all_predicted.reset_index(drop=True, inplace=True)\n",
    "    vc = pd.value_counts(all_predicted == all_labels)\n",
    "    try:\n",
    "        acc = vc[1]/len(all_labels)\n",
    "    except:\n",
    "        if(vc.index[0]==False):\n",
    "            acc = 0\n",
    "        else:\n",
    "            acc = 1\n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_foldername = 'VB-CNN_for_md_%s'%(mydataset)\n",
    "except FileExistsError:\n",
    "    print('Folder exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.....\n",
      "Epoch 1 - Training acc: 0.4981 -Training loss: 0.6924 - Val acc: 0.5039 - Val loss: 0.6843 - Time: 33.7624s \n",
      "Epoch 2 - Training acc: 0.5343 -Training loss: 0.6838 - Val acc: 0.5537 - Val loss: 0.6733 - Time: 33.8325s \n",
      "Epoch 3 - Training acc: 0.5601 -Training loss: 0.6728 - Val acc: 0.5671 - Val loss: 0.6573 - Time: 34.0663s \n",
      "Epoch 4 - Training acc: 0.5928 -Training loss: 0.6501 - Val acc: 0.5994 - Val loss: 0.6315 - Time: 34.1782s \n",
      "Epoch 5 - Training acc: 0.6257 -Training loss: 0.6191 - Val acc: 0.6203 - Val loss: 0.6175 - Time: 34.1096s \n",
      "Epoch 6 - Training acc: 0.6461 -Training loss: 0.5936 - Val acc: 0.6306 - Val loss: 0.6144 - Time: 34.0999s \n",
      "Epoch 7 - Training acc: 0.6694 -Training loss: 0.5661 - Val acc: 0.6487 - Val loss: 0.6067 - Time: 33.8322s \n",
      "Epoch 8 - Training acc: 0.6911 -Training loss: 0.5374 - Val acc: 0.6451 - Val loss: 0.6165 - Time: 33.7489s \n",
      "Epoch 9 - Training acc: 0.7105 -Training loss: 0.5127 - Val acc: 0.6600 - Val loss: 0.6154 - Time: 33.7130s \n",
      "Epoch 10 - Training acc: 0.7257 -Training loss: 0.4920 - Val acc: 0.6493 - Val loss: 0.6309 - Time: 33.6126s \n",
      "Epoch 11 - Training acc: 0.7371 -Training loss: 0.4773 - Val acc: 0.6613 - Val loss: 0.6935 - Time: 34.0105s \n",
      "Epoch 12 - Training acc: 0.7545 -Training loss: 0.4500 - Val acc: 0.6564 - Val loss: 0.7837 - Time: 33.8554s \n",
      "Epoch 13 - Training acc: 0.7667 -Training loss: 0.4364 - Val acc: 0.6624 - Val loss: 0.7502 - Time: 33.9241s \n",
      "Epoch 14 - Training acc: 0.7782 -Training loss: 0.4156 - Val acc: 0.6610 - Val loss: 0.7518 - Time: 33.7218s \n",
      "Epoch 15 - Training acc: 0.7848 -Training loss: 0.4026 - Val acc: 0.6634 - Val loss: 0.7862 - Time: 33.4790s \n",
      "Epoch 16 - Training acc: 0.7944 -Training loss: 0.3912 - Val acc: 0.6613 - Val loss: 0.8790 - Time: 33.4891s \n",
      "Epoch 17 - Training acc: 0.7989 -Training loss: 0.3846 - Val acc: 0.6634 - Val loss: 0.8062 - Time: 33.5594s \n",
      "Epoch 18 - Training acc: 0.8041 -Training loss: 0.3775 - Val acc: 0.6490 - Val loss: 0.9879 - Time: 33.7677s \n",
      "Epoch 19 - Training acc: 0.8068 -Training loss: 0.3655 - Val acc: 0.6567 - Val loss: 1.1434 - Time: 33.7825s \n",
      "Epoch 20 - Training acc: 0.7993 -Training loss: 0.3743 - Val acc: 0.6578 - Val loss: 1.1198 - Time: 33.7447s \n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "print('Training started.....')\n",
    "\n",
    "EPOCHS=20\n",
    "BEST_VAL = 9999.9\n",
    "BEST_MODEL = None\n",
    "BEST_EPOCH = None\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    running_acc = 0\n",
    "    running_loss = 0\n",
    "    timer = time.time()\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_iterator:\n",
    "        batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch.codes)\n",
    "        loss = criterion(output, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = softmax_accuracy(output,batch.label)\n",
    "        running_acc += acc\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        running_acc_val = 0\n",
    "        running_loss_val = 0\n",
    "        for batch in valid_iterator:\n",
    "            batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "            output_val = model(batch.codes)\n",
    "            loss_val = criterion(output_val,batch.label)\n",
    "            acc_val = softmax_accuracy(output_val,batch.label)\n",
    "            running_acc_val += acc_val\n",
    "            running_loss_val += loss_val.item()\n",
    "\n",
    "    print_out = \"Epoch %d - Training acc: %.4f -Training loss: %.4f - Val acc: %.4f - Val loss: %.4f - Time: %.4fs \\n\" % (e+1,\n",
    "    running_acc/len(train_iterator),\n",
    "    running_loss/len(train_iterator),\n",
    "    running_acc_val/len(valid_iterator),\n",
    "    running_loss_val/len(valid_iterator),\n",
    "    (time.time()-timer))\n",
    "    \n",
    "    \n",
    "    selected_model = False\n",
    "    \n",
    "    if selected_model:\n",
    "        \n",
    "        myfile = open(\"res.txt\", \"a\")\n",
    "\n",
    "        if (running_loss_val/len(valid_iterator)) < BEST_VAL:\n",
    "            print('Val_loss decreased!')\n",
    "            print(print_out, end='')\n",
    "            myfile.write('Val_loss decreased!')\n",
    "            myfile.write(print_out)\n",
    "\n",
    "            BEST_VAL = (running_loss_val/len(valid_iterator))\n",
    "            BEST_MODEL = copy.deepcopy(model)\n",
    "            BEST_EPOCH = e+1\n",
    "            model_name = 'models/%s/model_ep_%d.tar' % (model_foldername,e+1)\n",
    "            torch.save({\n",
    "                'epoch': e+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss}, model_name)\n",
    "\n",
    "        else:\n",
    "            print(print_out, end='')\n",
    "            myfile.write(print_out)\n",
    "\n",
    "        myfile.close()\n",
    "        \n",
    "    else:\n",
    "        print(print_out, end='')\n",
    "        model_name = 'models/%s/model_ep_%d.tar' % (model_foldername,e+1)\n",
    "        torch.save({\n",
    "            'epoch': e+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss}, model_name)\n",
    "\n",
    "        \n",
    "\n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_testing(all_pred, all_labels):\n",
    "    def getClass(x):\n",
    "        return(x.index(max(x)))\n",
    "\n",
    "    probs = pd.Series(all_pred)\n",
    "    all_predicted = probs.apply(getClass)\n",
    "    all_predicted.reset_index(drop=True, inplace=True)\n",
    "    vc = pd.value_counts(all_predicted == all_labels)\n",
    "\n",
    "    probs2=[]\n",
    "    for x in probs:\n",
    "        probs2.append(x[1])\n",
    "\n",
    "    confusion = sklearn.metrics.confusion_matrix(y_true=all_labels, y_pred=all_predicted)\n",
    "    print('Confusion matrix: \\n',confusion)\n",
    "\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion.ravel()\n",
    "        print('\\nTP:',tp)\n",
    "        print('FP:',fp)\n",
    "        print('TN:',tn)\n",
    "        print('FN:',fn)\n",
    "\n",
    "        ## Performance measure\n",
    "        print('\\nAccuracy: '+ str(sklearn.metrics.accuracy_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "        print('Precision: '+ str(sklearn.metrics.precision_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "        print('F-measure: '+ str(sklearn.metrics.f1_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "        print('Recall: '+ str(sklearn.metrics.recall_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "        print('Precision-Recall AUC: '+ str(sklearn.metrics.average_precision_score(y_true=all_labels, y_score=probs2)))\n",
    "        print('AUC: '+ str(sklearn.metrics.roc_auc_score(y_true=all_labels, y_score=probs2)))\n",
    "        print('MCC: '+ str(sklearn.metrics.matthews_corrcoef(y_true=all_labels, y_pred=all_predicted)))\n",
    "    except:\n",
    "        None\n",
    "        print('This is multiclass prediction')\n",
    "    return(all_predicted)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing started.......\n",
      "Confusion matrix: \n",
      " [[967 510]\n",
      " [493 762]]\n",
      "\n",
      "TP: 762\n",
      "FP: 510\n",
      "TN: 967\n",
      "FN: 493\n",
      "\n",
      "Accuracy: 0.6328696925329429\n",
      "Precision: 0.5990566037735849\n",
      "F-measure: 0.6030866640284923\n",
      "Recall: 0.6071713147410358\n",
      "Precision-Recall AUC: 0.6828537632845266\n",
      "AUC: 0.7026925473461605\n",
      "MCC: 0.2616309694810066\n"
     ]
    }
   ],
   "source": [
    "print('Testing started.......')\n",
    "## Testing\n",
    "checkpoint = torch.load('models/VB-CNN_for_md_devign/model_ep_7.tar', map_location='cuda')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    running_acc_test = 0\n",
    "    running_loss_test = 0\n",
    "    all_pred=[]\n",
    "    all_labels=[]\n",
    "    for batch in test_iterator:\n",
    "        batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "        output_test = model(batch.codes).squeeze(1)\n",
    "        loss_test = criterion(output_test,batch.label)\n",
    "        acc_test = softmax_accuracy(output_test,batch.label)\n",
    "        running_acc_test += acc_test\n",
    "        running_loss_test += loss_test.item()\n",
    "        all_pred += output_test.tolist()\n",
    "        all_labels += batch.label.tolist()\n",
    "\n",
    "ap=evaluate_testing(all_pred, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use below only on MVD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "           non-vulnerable   0.662329  0.654705  0.658495      1477\n",
      "                  CWE-404   0.599057  0.607171  0.603087      1255\n",
      "                  CWE-476   0.000000  0.000000  0.000000         0\n",
      "                  CWE-119   0.000000  0.000000  0.000000         0\n",
      "                  CWE-706   0.000000  0.000000  0.000000         0\n",
      "                  CWE-670   0.000000  0.000000  0.000000         0\n",
      "                  CWE-673   0.000000  0.000000  0.000000         0\n",
      "CWE-119, CWE-666, CWE-573   0.000000  0.000000  0.000000         0\n",
      "                  CWE-573   0.000000  0.000000  0.000000         0\n",
      "                  CWE-668   0.000000  0.000000  0.000000         0\n",
      "CWE-400, CWE-665, CWE-020   0.000000  0.000000  0.000000         0\n",
      "                  CWE-662   0.000000  0.000000  0.000000         0\n",
      "                  CWE-400   0.000000  0.000000  0.000000         0\n",
      "                  CWE-665   0.000000  0.000000  0.000000         0\n",
      "                  CWE-020   0.000000  0.000000  0.000000         0\n",
      "                  CWE-074   0.000000  0.000000  0.000000         0\n",
      "                  CWE-362   0.000000  0.000000  0.000000         0\n",
      "                  CWE-191   0.000000  0.000000  0.000000         0\n",
      "                  CWE-190   0.000000  0.000000  0.000000         0\n",
      "                  CWE-610   0.000000  0.000000  0.000000         0\n",
      "                  CWE-704   0.000000  0.000000  0.000000         0\n",
      "                  CWE-170   0.000000  0.000000  0.000000         0\n",
      "                  CWE-676   0.000000  0.000000  0.000000         0\n",
      "                  CWE-187   0.000000  0.000000  0.000000         0\n",
      "                  CWE-138   0.000000  0.000000  0.000000         0\n",
      "                  CWE-369   0.000000  0.000000  0.000000         0\n",
      "         CWE-662, CWE-573   0.000000  0.000000  0.000000         0\n",
      "                  CWE-834   0.000000  0.000000  0.000000         0\n",
      "         CWE-400, CWE-665   0.000000  0.000000  0.000000         0\n",
      "         CWE-400, CWE-404   0.000000  0.000000  0.000000         0\n",
      "                  CWE-221   0.000000  0.000000  0.000000         0\n",
      "                  CWE-754   0.000000  0.000000  0.000000         0\n",
      "                  CWE-311   0.000000  0.000000  0.000000         0\n",
      "         CWE-404, CWE-668   0.000000  0.000000  0.000000         0\n",
      "                  CWE-506   0.000000  0.000000  0.000000         0\n",
      "                  CWE-758   0.000000  0.000000  0.000000         0\n",
      "                  CWE-666   0.000000  0.000000  0.000000         0\n",
      "                  CWE-467   0.000000  0.000000  0.000000         0\n",
      "                  CWE-327   0.000000  0.000000  0.000000         0\n",
      "         CWE-666, CWE-573   0.000000  0.000000  0.000000         0\n",
      "                  CWE-469   0.000000  0.000000  0.000000         0\n",
      "\n",
      "                micro avg   0.632870  0.632870  0.632870      2732\n",
      "                macro avg   0.030765  0.030777  0.030770      2732\n",
      "             weighted avg   0.633263  0.632870  0.633042      2732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tn=['non-vulnerable','CWE-404','CWE-476','CWE-119','CWE-706','CWE-670','CWE-673','CWE-119, CWE-666, CWE-573','CWE-573','CWE-668','CWE-400, CWE-665, CWE-020','CWE-662','CWE-400','CWE-665','CWE-020','CWE-074','CWE-362','CWE-191','CWE-190','CWE-610','CWE-704','CWE-170','CWE-676','CWE-187','CWE-138','CWE-369','CWE-662, CWE-573','CWE-834','CWE-400, CWE-665','CWE-400, CWE-404','CWE-221','CWE-754','CWE-311','CWE-404, CWE-668','CWE-506','CWE-758','CWE-666','CWE-467','CWE-327','CWE-666, CWE-573','CWE-469']\n",
    "report = sklearn.metrics.classification_report(y_true=all_labels, y_pred=ap, digits=6,labels=np.arange(0,41),target_names=tn)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TP: 967\n",
      "FP: 493\n",
      "TN: 762\n",
      "FN: 510\n"
     ]
    }
   ],
   "source": [
    "confusion = sklearn.metrics.confusion_matrix(y_true=[1 if x == 0 else 0 for x in all_labels], y_pred=[1 if x == 0 else 0 for x in ap])\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "print('\\nTP:',tp)\n",
    "print('FP:',fp)\n",
    "print('TN:',tn)\n",
    "print('FN:',fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-07062eb5d785>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0maug_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mconfusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_y_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_y_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mall_fpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## FPR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mw_all_fpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0maug_y_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## w_FPR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "all_fpr = []\n",
    "w_all_fpr = []\n",
    "aug_y_true_sum = 0\n",
    "for counter in range(41):\n",
    "    aug_y_true = [1 if x == counter else 0 for x in all_labels]\n",
    "    aug_y_pred = [1 if x == counter else 0 for x in ap]\n",
    "    confusion = sklearn.metrics.confusion_matrix(y_true=aug_y_true, y_pred=aug_y_pred)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "    all_fpr.append(fp/(fp+tn))  ## FPR\n",
    "    w_all_fpr.append((fp/(fp+tn))*aug_y_true.count(1))  ## w_FPR\n",
    "    aug_y_true_sum += aug_y_true.count(1)\n",
    "\n",
    "print('FPR: ', sum(all_fpr)/41.0*100.0)\n",
    "print('Weighted FPR: ', sum(w_all_fpr)/aug_y_true_sum*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
