## Environment

> joern 0.3.1（jdk 1.7）, neo4j 2.1.5, python 3.6, tensorflow 1.6, gensim 3.4

## Step 1: Generating slices (i.e., SeVCs)

1. Use joern to parse source code: the input is source code files, and the output is a file named .joernIndex.

2. get_cfg_relation.py: This file is used to get CFG graphs of functions using joern tool. The input is output of the first step, and the outputs are stored with folders in cfg_db. 

3. complete_PDG.py: This file is used to get PDG graph of functions. The inputs are files in cfg_db, and the outputs are stored with folders in pdg_db.

4. access_db_operate.py: This file is used to get the call graph of functions. The inputs are files in pdg_db, and the outputs are stored with folders in dict_call2cfgNodeID_funcID.

5. points_get.py: This file is used to get four kinds of SyVCs. The inputs are files in dict_call2cfgNodeID_funcID, and the outputs are four kinds of SyVCs.

6. extract_df.py: This file is used to extract slices. The inputs are files generated by points_get.py, and the outputs are slice files.

7. dealfile.py: This file is used to get the line numbers of vulnerable lines in nvd dataset.The input are source code files or func files and diff files.

8. getVulLineForCounting.py: 

> python getVulLineForCounting.py ./000 ./SARD_testcaseinfo.xml

This file is used to extract the line numbers of vulnerable lines from SARD_testcaseinfo.xml. 
"000" is the source code file. The output is SARD_testcaseinfo.txt, and then renamed as contain_all.txt.

9. make_label_sard.py: This file is used to get labels of sard slices.

10. make_label_nvd.py: This file is used to get labels of nvd slices.

11. data_preprocess.py: This file is used to write the labels to the slice files.

## Step 2: Data preprocess

1. create_hash.py: This file is used to get the hash value of slices. The input is slice file generated by extract_df.py,and the output is hashlist of slices.

2. delete_list.py: This file is used to get index of slices that needed to be delete.  The input is hashlist generated by create_hash.py,and the output is list_delete.

3. process_dataflow_func.py: This file is used to process the slices, including read the pkl file and split codes into corpus. The inputs are the slice file and the label file generated by extract_df.py and make_label_sard.py(make_label_nvd.py), and the output is the corpus file named with testcase id.

4. create_w2vmodel.py: This file is used to train word2vec model. The inputs are corpus files, and the output is the word2vec model.

5. get_dl_input.py: This file is used to convert tokens of slices in corpus files into vectors by trained word2vec model. The input is the trained word2vec model and corpus files, and the outputs are vector files.

6. dealrawdata.py: This file is used to make vectors generated by get_dl_input.py into fixed length. 

## Step 3: Deep learning model

1. bgru.py: This file is used to train BGRU model and get test results. The inputs are vector files generated by dealrawdata.py, and the output is trained model and test results.

2. preprocess_dl_Input_version5.py: This file is used to preprocess data imported into model. It is imported by bgru.py.
